
==> Audit <==
|-----------|-------------------------|----------|--------------|---------|----------------------|----------------------|
|  Command  |          Args           | Profile  |     User     | Version |      Start Time      |       End Time       |
|-----------|-------------------------|----------|--------------|---------|----------------------|----------------------|
| start     | --driver=docker         | minikube | ismailyasser | v1.35.0 | 20 Feb 25 20:19 EET  |                      |
| start     | --driver=docker         | minikube | ismailyasser | v1.35.0 | 20 Feb 25 20:19 EET  |                      |
| start     | --driver=docker         | minikube | ismailyasser | v1.35.0 | 20 Feb 25 20:27 EET  |                      |
| start     | --driver=docker         | minikube | ismailyasser | v1.35.0 | 20 Feb 25 20:27 EET  |                      |
| start     | --driver=docker         | minikube | ismailyasser | v1.35.0 | 20 Feb 25 20:28 EET  |                      |
| start     | --driver=docker         | minikube | ismailyasser | v1.35.0 | 20 Feb 25 20:29 EET  |                      |
| start     |                         | minikube | ismailyasser | v1.35.0 | 20 Feb 25 20:30 EET  | 20 Feb 25 20:40 EET  |
| config    | get driver              | minikube | ismailyasser | v1.35.0 | 20 Feb 25 20:42 EET  |                      |
| config    | set driver docker       | minikube | ismailyasser | v1.35.0 | 20 Feb 25 20:42 EET  | 20 Feb 25 20:42 EET  |
| start     | --driver=docker         | minikube | ismailyasser | v1.35.0 | 20 Feb 25 20:42 EET  | 20 Feb 25 20:42 EET  |
| dashboard |                         | minikube | ismailyasser | v1.35.0 | 20 Feb 25 20:44 EET  |                      |
| start     |                         | minikube | ismailyasser | v1.35.0 | 20 Feb 25 20:51 EET  | 20 Feb 25 20:51 EET  |
| dashboard |                         | minikube | ismailyasser | v1.35.0 | 20 Feb 25 20:53 EET  |                      |
| service   | hello-node              | minikube | ismailyasser | v1.35.0 | 20 Feb 25 20:59 EET  |                      |
| service   | hello-node              | minikube | ismailyasser | v1.35.0 | 20 Feb 25 21:00 EET  |                      |
| dashboard |                         | minikube | ismailyasser | v1.35.0 | 20 Feb 25 21:08 EET  |                      |
| start     |                         | minikube | ismailyasser | v1.35.0 | 25 Feb 25 15:25 EET  | 25 Feb 25 15:26 EET  |
| start     |                         | minikube | ismailyasser | v1.35.0 | 21 May 25 18:33 EEST | 21 May 25 18:34 EEST |
| ip        |                         | minikube | ismailyasser | v1.35.0 | 21 May 25 18:43 EEST | 21 May 25 18:43 EEST |
| dashboard |                         | minikube | ismailyasser | v1.35.0 | 21 May 25 18:49 EEST |                      |
| service   | frontend-service        | minikube | ismailyasser | v1.35.0 | 21 May 25 19:01 EEST |                      |
| service   | frontend-service --url  | minikube | ismailyasser | v1.35.0 | 21 May 25 19:03 EEST | 21 May 25 19:04 EEST |
| tunnel    |                         | minikube | ismailyasser | v1.35.0 | 21 May 25 19:04 EEST | 21 May 25 19:04 EEST |
| start     |                         | minikube | ismailyasser | v1.35.0 | 21 May 25 19:12 EEST | 21 May 25 19:13 EEST |
| start     |                         | minikube | ismailyasser | v1.35.0 | 21 May 25 20:18 EEST | 21 May 25 20:20 EEST |
| start     |                         | minikube | ismailyasser | v1.35.0 | 23 May 25 02:26 EEST | 23 May 25 02:26 EEST |
| service   | frontend                | minikube | ismailyasser | v1.35.0 | 23 May 25 02:28 EEST |                      |
| stop      |                         | minikube | ismailyasser | v1.35.0 | 23 May 25 02:30 EEST | 23 May 25 02:31 EEST |
| config    | set memory 4096         | minikube | ismailyasser | v1.35.0 | 23 May 25 02:31 EEST | 23 May 25 02:31 EEST |
| config    | set cpus 2              | minikube | ismailyasser | v1.35.0 | 23 May 25 02:31 EEST | 23 May 25 02:31 EEST |
| start     |                         | minikube | ismailyasser | v1.35.0 | 23 May 25 02:31 EEST | 23 May 25 02:31 EEST |
| service   | frontend                | minikube | ismailyasser | v1.35.0 | 23 May 25 02:32 EEST |                      |
| ssh       |                         | minikube | ismailyasser | v1.35.0 | 23 May 25 02:39 EEST |                      |
| stop      |                         | minikube | ismailyasser | v1.35.0 | 23 May 25 02:40 EEST | 23 May 25 02:40 EEST |
| start     |                         | minikube | ismailyasser | v1.35.0 | 23 May 25 02:40 EEST | 23 May 25 02:41 EEST |
| addons    | enable metrics-server   | minikube | ismailyasser | v1.35.0 | 23 May 25 02:41 EEST | 23 May 25 02:41 EEST |
| image     | load frontend:local     | minikube | ismailyasser | v1.35.0 | 23 May 25 02:53 EEST | 23 May 25 02:54 EEST |
| image     | load help-service:local | minikube | ismailyasser | v1.35.0 | 23 May 25 02:54 EEST | 23 May 25 02:57 EEST |
| image     | load user-service:local | minikube | ismailyasser | v1.35.0 | 23 May 25 02:57 EEST | 23 May 25 02:59 EEST |
| start     |                         | minikube | ismailyasser | v1.35.0 | 23 May 25 03:08 EEST | 23 May 25 03:09 EEST |
|-----------|-------------------------|----------|--------------|---------|----------------------|----------------------|


==> Last Start <==
Log file created at: 2025/05/23 03:08:15
Running on machine: DESKTOP-N1RKT5T
Binary: Built with gc go1.23.4 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0523 03:08:15.862418   10489 out.go:345] Setting OutFile to fd 1 ...
I0523 03:08:15.862533   10489 out.go:397] isatty.IsTerminal(1) = true
I0523 03:08:15.862537   10489 out.go:358] Setting ErrFile to fd 2...
I0523 03:08:15.862541   10489 out.go:397] isatty.IsTerminal(2) = true
I0523 03:08:15.862678   10489 root.go:338] Updating PATH: /home/ismailyasser/.minikube/bin
I0523 03:08:15.862915   10489 out.go:352] Setting JSON to false
I0523 03:08:15.863502   10489 start.go:129] hostinfo: {"hostname":"DESKTOP-N1RKT5T","uptime":29392,"bootTime":1747929503,"procs":36,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"5.15.167.4-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"39afda6d-5c4d-4c1a-8f20-5585c0184eaf"}
I0523 03:08:15.863541   10489 start.go:139] virtualization:  guest
I0523 03:08:15.887931   10489 out.go:177] üòÑ  minikube v1.35.0 on Ubuntu 24.04 (amd64)
I0523 03:08:15.893179   10489 notify.go:220] Checking for updates...
I0523 03:08:15.894477   10489 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0523 03:08:15.895816   10489 driver.go:394] Setting default libvirt URI to qemu:///system
I0523 03:08:16.251036   10489 docker.go:123] docker version: linux-28.1.1:Docker Engine - Community
I0523 03:08:16.251147   10489 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0523 03:08:17.955968   10489 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.70478924s)
I0523 03:08:17.956502   10489 info.go:266] docker info: {ID:d59e9f67-0f09-4972-aaa7-385cd74a88f0 Containers:3 ContainersRunning:1 ContainersPaused:0 ContainersStopped:2 Images:6 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:31 OomKillDisable:true NGoroutines:48 SystemTime:2025-05-23 03:08:17.944405699 +0300 EEST LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Ubuntu 24.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:10345271296 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:DESKTOP-N1RKT5T Labels:[] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/usr/local/lib/docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.7] map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-buildx] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0-desktop.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1-desktop.1] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:/usr/local/lib/docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.8] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-feedback: no such file or directory Name:feedback Path:/usr/local/lib/docker/cli-plugins/docker-feedback] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.1]] Warnings:<nil>}}
I0523 03:08:17.956637   10489 docker.go:318] overlay module found
I0523 03:08:17.960964   10489 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0523 03:08:17.963520   10489 start.go:297] selected driver: docker
I0523 03:08:17.963529   10489 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2400 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/ismailyasser:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0523 03:08:17.963607   10489 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0523 03:08:17.963685   10489 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0523 03:08:18.930837   10489 info.go:266] docker info: {ID:d59e9f67-0f09-4972-aaa7-385cd74a88f0 Containers:3 ContainersRunning:1 ContainersPaused:0 ContainersStopped:2 Images:6 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:31 OomKillDisable:true NGoroutines:48 SystemTime:2025-05-23 03:08:18.914700197 +0300 EEST LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Ubuntu 24.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:10345271296 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:DESKTOP-N1RKT5T Labels:[] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/usr/local/lib/docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.7] map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-buildx] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0-desktop.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1-desktop.1] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:/usr/local/lib/docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.8] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-feedback: no such file or directory Name:feedback Path:/usr/local/lib/docker/cli-plugins/docker-feedback] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.1]] Warnings:<nil>}}
I0523 03:08:18.932037   10489 cni.go:84] Creating CNI manager for ""
I0523 03:08:18.932104   10489 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0523 03:08:18.932250   10489 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2400 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/ismailyasser:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0523 03:08:18.940127   10489 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0523 03:08:18.944453   10489 cache.go:121] Beginning downloading kic base image for docker with docker
I0523 03:08:18.950034   10489 out.go:177] üöú  Pulling base image v0.0.46 ...
I0523 03:08:18.955951   10489 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0523 03:08:18.956096   10489 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0523 03:08:18.956143   10489 preload.go:146] Found local preload: /home/ismailyasser/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0523 03:08:18.956152   10489 cache.go:56] Caching tarball of preloaded images
I0523 03:08:18.957531   10489 preload.go:172] Found /home/ismailyasser/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0523 03:08:18.957552   10489 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0523 03:08:18.957769   10489 profile.go:143] Saving config to /home/ismailyasser/.minikube/profiles/minikube/config.json ...
I0523 03:08:19.278459   10489 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon, skipping pull
I0523 03:08:19.278470   10489 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in daemon, skipping load
I0523 03:08:19.278505   10489 cache.go:227] Successfully downloaded all kic artifacts
I0523 03:08:19.278532   10489 start.go:360] acquireMachinesLock for minikube: {Name:mka96c693f9d898f6f206ecd49c3a7c23ef8bb03 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0523 03:08:19.278628   10489 start.go:364] duration metric: took 68.933¬µs to acquireMachinesLock for "minikube"
I0523 03:08:19.278647   10489 start.go:96] Skipping create...Using existing machine configuration
I0523 03:08:19.278651   10489 fix.go:54] fixHost starting: 
I0523 03:08:19.278859   10489 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0523 03:08:19.512331   10489 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0523 03:08:19.512350   10489 fix.go:138] unexpected machine state, will restart: <nil>
I0523 03:08:19.517463   10489 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0523 03:08:19.521983   10489 cli_runner.go:164] Run: docker start minikube
I0523 03:08:24.600819   10489 cli_runner.go:217] Completed: docker start minikube: (2.351940741s)
I0523 03:08:24.600923   10489 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0523 03:08:24.963781   10489 kic.go:430] container "minikube" state is running.
I0523 03:08:24.964377   10489 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0523 03:08:25.276467   10489 profile.go:143] Saving config to /home/ismailyasser/.minikube/profiles/minikube/config.json ...
I0523 03:08:25.276725   10489 machine.go:93] provisionDockerMachine start ...
I0523 03:08:25.276806   10489 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 03:08:25.680118   10489 main.go:141] libmachine: Using SSH client type: native
I0523 03:08:25.680358   10489 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0523 03:08:25.680367   10489 main.go:141] libmachine: About to run SSH command:
hostname
I0523 03:08:25.869222   10489 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0523 03:08:25.869257   10489 ubuntu.go:169] provisioning hostname "minikube"
I0523 03:08:25.869372   10489 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 03:08:26.281343   10489 main.go:141] libmachine: Using SSH client type: native
I0523 03:08:26.281607   10489 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0523 03:08:26.281618   10489 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0523 03:08:26.508049   10489 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0523 03:08:26.508124   10489 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 03:08:26.837902   10489 main.go:141] libmachine: Using SSH client type: native
I0523 03:08:26.838115   10489 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0523 03:08:26.838130   10489 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0523 03:08:26.999243   10489 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0523 03:08:26.999274   10489 ubuntu.go:175] set auth options {CertDir:/home/ismailyasser/.minikube CaCertPath:/home/ismailyasser/.minikube/certs/ca.pem CaPrivateKeyPath:/home/ismailyasser/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/ismailyasser/.minikube/machines/server.pem ServerKeyPath:/home/ismailyasser/.minikube/machines/server-key.pem ClientKeyPath:/home/ismailyasser/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/ismailyasser/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/ismailyasser/.minikube}
I0523 03:08:26.999312   10489 ubuntu.go:177] setting up certificates
I0523 03:08:26.999326   10489 provision.go:84] configureAuth start
I0523 03:08:26.999411   10489 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0523 03:08:27.511300   10489 provision.go:143] copyHostCerts
I0523 03:08:27.511361   10489 exec_runner.go:144] found /home/ismailyasser/.minikube/ca.pem, removing ...
I0523 03:08:27.511382   10489 exec_runner.go:203] rm: /home/ismailyasser/.minikube/ca.pem
I0523 03:08:27.511479   10489 exec_runner.go:151] cp: /home/ismailyasser/.minikube/certs/ca.pem --> /home/ismailyasser/.minikube/ca.pem (1094 bytes)
I0523 03:08:27.511762   10489 exec_runner.go:144] found /home/ismailyasser/.minikube/cert.pem, removing ...
I0523 03:08:27.511775   10489 exec_runner.go:203] rm: /home/ismailyasser/.minikube/cert.pem
I0523 03:08:27.511841   10489 exec_runner.go:151] cp: /home/ismailyasser/.minikube/certs/cert.pem --> /home/ismailyasser/.minikube/cert.pem (1139 bytes)
I0523 03:08:27.511939   10489 exec_runner.go:144] found /home/ismailyasser/.minikube/key.pem, removing ...
I0523 03:08:27.511946   10489 exec_runner.go:203] rm: /home/ismailyasser/.minikube/key.pem
I0523 03:08:27.511998   10489 exec_runner.go:151] cp: /home/ismailyasser/.minikube/certs/key.pem --> /home/ismailyasser/.minikube/key.pem (1679 bytes)
I0523 03:08:27.512076   10489 provision.go:117] generating server cert: /home/ismailyasser/.minikube/machines/server.pem ca-key=/home/ismailyasser/.minikube/certs/ca.pem private-key=/home/ismailyasser/.minikube/certs/ca-key.pem org=ismailyasser.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0523 03:08:28.080320   10489 provision.go:177] copyRemoteCerts
I0523 03:08:28.080416   10489 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0523 03:08:28.080482   10489 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 03:08:28.356547   10489 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/ismailyasser/.minikube/machines/minikube/id_rsa Username:docker}
I0523 03:08:28.475127   10489 ssh_runner.go:362] scp /home/ismailyasser/.minikube/machines/server.pem --> /etc/docker/server.pem (1196 bytes)
I0523 03:08:28.514664   10489 ssh_runner.go:362] scp /home/ismailyasser/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0523 03:08:28.547326   10489 ssh_runner.go:362] scp /home/ismailyasser/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1094 bytes)
I0523 03:08:28.575002   10489 provision.go:87] duration metric: took 1.575662784s to configureAuth
I0523 03:08:28.575020   10489 ubuntu.go:193] setting minikube options for container-runtime
I0523 03:08:28.575183   10489 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0523 03:08:28.575228   10489 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 03:08:28.839246   10489 main.go:141] libmachine: Using SSH client type: native
I0523 03:08:28.839445   10489 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0523 03:08:28.839454   10489 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0523 03:08:29.003313   10489 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0523 03:08:29.003339   10489 ubuntu.go:71] root file system type: overlay
I0523 03:08:29.003604   10489 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0523 03:08:29.003728   10489 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 03:08:29.253454   10489 main.go:141] libmachine: Using SSH client type: native
I0523 03:08:29.253637   10489 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0523 03:08:29.253703   10489 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0523 03:08:29.427400   10489 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0523 03:08:29.427516   10489 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 03:08:29.734022   10489 main.go:141] libmachine: Using SSH client type: native
I0523 03:08:29.734192   10489 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0523 03:08:29.734206   10489 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0523 03:08:29.897553   10489 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0523 03:08:29.897575   10489 machine.go:96] duration metric: took 4.620840252s to provisionDockerMachine
I0523 03:08:29.897591   10489 start.go:293] postStartSetup for "minikube" (driver="docker")
I0523 03:08:29.897608   10489 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0523 03:08:29.897691   10489 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0523 03:08:29.897757   10489 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 03:08:30.124489   10489 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/ismailyasser/.minikube/machines/minikube/id_rsa Username:docker}
I0523 03:08:30.219799   10489 ssh_runner.go:195] Run: cat /etc/os-release
I0523 03:08:30.223273   10489 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0523 03:08:30.223317   10489 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0523 03:08:30.223329   10489 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0523 03:08:30.223335   10489 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0523 03:08:30.223344   10489 filesync.go:126] Scanning /home/ismailyasser/.minikube/addons for local assets ...
I0523 03:08:30.223389   10489 filesync.go:126] Scanning /home/ismailyasser/.minikube/files for local assets ...
I0523 03:08:30.223404   10489 start.go:296] duration metric: took 325.806396ms for postStartSetup
I0523 03:08:30.223451   10489 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0523 03:08:30.223494   10489 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 03:08:30.443646   10489 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/ismailyasser/.minikube/machines/minikube/id_rsa Username:docker}
I0523 03:08:30.554811   10489 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0523 03:08:30.568460   10489 fix.go:56] duration metric: took 8.562861676s for fixHost
I0523 03:08:30.568496   10489 start.go:83] releasing machines lock for "minikube", held for 8.562995076s
I0523 03:08:30.568622   10489 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0523 03:08:30.839487   10489 ssh_runner.go:195] Run: cat /version.json
I0523 03:08:30.839532   10489 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 03:08:30.839563   10489 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0523 03:08:30.839619   10489 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 03:08:31.010842   10489 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/ismailyasser/.minikube/machines/minikube/id_rsa Username:docker}
I0523 03:08:31.011903   10489 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/ismailyasser/.minikube/machines/minikube/id_rsa Username:docker}
I0523 03:08:31.642905   10489 ssh_runner.go:195] Run: systemctl --version
I0523 03:08:31.665840   10489 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0523 03:08:31.679211   10489 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0523 03:08:31.733072   10489 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0523 03:08:31.733229   10489 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0523 03:08:31.754940   10489 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0523 03:08:31.754971   10489 start.go:495] detecting cgroup driver to use...
I0523 03:08:31.755022   10489 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0523 03:08:31.755216   10489 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0523 03:08:31.809692   10489 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0523 03:08:31.840099   10489 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0523 03:08:31.855952   10489 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0523 03:08:31.856016   10489 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0523 03:08:31.868975   10489 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0523 03:08:31.880770   10489 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0523 03:08:31.892824   10489 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0523 03:08:31.903315   10489 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0523 03:08:31.915816   10489 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0523 03:08:31.926434   10489 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0523 03:08:31.938728   10489 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0523 03:08:31.952093   10489 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0523 03:08:31.962670   10489 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0523 03:08:31.973442   10489 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0523 03:08:32.102960   10489 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0523 03:08:32.262647   10489 start.go:495] detecting cgroup driver to use...
I0523 03:08:32.262691   10489 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0523 03:08:32.262761   10489 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0523 03:08:32.283041   10489 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0523 03:08:32.283147   10489 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0523 03:08:32.303643   10489 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0523 03:08:32.341010   10489 ssh_runner.go:195] Run: which cri-dockerd
I0523 03:08:32.348530   10489 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0523 03:08:32.371072   10489 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0523 03:08:32.415908   10489 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0523 03:08:32.652754   10489 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0523 03:08:32.815988   10489 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0523 03:08:32.816137   10489 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0523 03:08:32.841244   10489 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0523 03:08:32.998711   10489 ssh_runner.go:195] Run: sudo systemctl restart docker
I0523 03:08:37.878924   10489 ssh_runner.go:235] Completed: sudo systemctl restart docker: (4.880189448s)
I0523 03:08:37.878966   10489 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0523 03:08:37.888173   10489 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0523 03:08:37.899037   10489 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0523 03:08:37.908485   10489 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0523 03:08:38.003119   10489 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0523 03:08:38.172260   10489 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0523 03:08:38.338289   10489 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0523 03:08:38.360649   10489 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0523 03:08:38.370037   10489 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0523 03:08:38.555702   10489 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0523 03:08:38.907167   10489 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0523 03:08:38.907235   10489 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0523 03:08:38.910722   10489 start.go:563] Will wait 60s for crictl version
I0523 03:08:38.910770   10489 ssh_runner.go:195] Run: which crictl
I0523 03:08:38.913755   10489 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0523 03:08:39.167669   10489 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0523 03:08:39.167793   10489 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0523 03:08:39.423911   10489 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0523 03:08:39.454001   10489 out.go:235] üê≥  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0523 03:08:39.454931   10489 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0523 03:08:39.677022   10489 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0523 03:08:39.680177   10489 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0523 03:08:39.690459   10489 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0523 03:08:39.878934   10489 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2400 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/ismailyasser:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0523 03:08:39.879084   10489 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0523 03:08:39.879153   10489 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0523 03:08:39.926113   10489 docker.go:689] Got preloaded images: -- stdout --
ismaill370/help-service:latest
ismaill370/user-service:<none>
ismaill370/frontend:latest
ismaill370/user-service:latest
ismaill370/help-service:<none>
curlimages/curl:latest
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
registry.k8s.io/e2e-test-images/agnhost:2.39
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0523 03:08:39.926128   10489 docker.go:619] Images already preloaded, skipping extraction
I0523 03:08:39.926184   10489 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0523 03:08:39.954535   10489 docker.go:689] Got preloaded images: -- stdout --
ismaill370/help-service:latest
ismaill370/user-service:<none>
ismaill370/frontend:latest
ismaill370/user-service:latest
ismaill370/help-service:<none>
curlimages/curl:latest
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
registry.k8s.io/e2e-test-images/agnhost:2.39
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0523 03:08:39.954550   10489 cache_images.go:84] Images are preloaded, skipping loading
I0523 03:08:39.954559   10489 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0523 03:08:39.954686   10489 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0523 03:08:39.954736   10489 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0523 03:08:40.262883   10489 cni.go:84] Creating CNI manager for ""
I0523 03:08:40.262897   10489 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0523 03:08:40.262906   10489 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0523 03:08:40.262938   10489 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0523 03:08:40.263053   10489 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0523 03:08:40.263099   10489 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0523 03:08:40.273317   10489 binaries.go:44] Found k8s binaries, skipping transfer
I0523 03:08:40.273360   10489 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0523 03:08:40.283657   10489 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0523 03:08:40.302450   10489 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0523 03:08:40.320796   10489 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0523 03:08:40.340174   10489 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0523 03:08:40.344057   10489 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0523 03:08:40.355174   10489 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0523 03:08:40.469807   10489 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0523 03:08:40.488243   10489 certs.go:68] Setting up /home/ismailyasser/.minikube/profiles/minikube for IP: 192.168.49.2
I0523 03:08:40.488256   10489 certs.go:194] generating shared ca certs ...
I0523 03:08:40.488291   10489 certs.go:226] acquiring lock for ca certs: {Name:mkb72fd804cf30ae62937869c5f965463f4522cf Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0523 03:08:40.488458   10489 certs.go:235] skipping valid "minikubeCA" ca cert: /home/ismailyasser/.minikube/ca.key
I0523 03:08:40.488507   10489 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/ismailyasser/.minikube/proxy-client-ca.key
I0523 03:08:40.488517   10489 certs.go:256] generating profile certs ...
I0523 03:08:40.488599   10489 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/ismailyasser/.minikube/profiles/minikube/client.key
I0523 03:08:40.489110   10489 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/ismailyasser/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0523 03:08:40.489155   10489 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/ismailyasser/.minikube/profiles/minikube/proxy-client.key
I0523 03:08:40.489252   10489 certs.go:484] found cert: /home/ismailyasser/.minikube/certs/ca-key.pem (1675 bytes)
I0523 03:08:40.489294   10489 certs.go:484] found cert: /home/ismailyasser/.minikube/certs/ca.pem (1094 bytes)
I0523 03:08:40.489317   10489 certs.go:484] found cert: /home/ismailyasser/.minikube/certs/cert.pem (1139 bytes)
I0523 03:08:40.489340   10489 certs.go:484] found cert: /home/ismailyasser/.minikube/certs/key.pem (1679 bytes)
I0523 03:08:40.489907   10489 ssh_runner.go:362] scp /home/ismailyasser/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0523 03:08:40.519346   10489 ssh_runner.go:362] scp /home/ismailyasser/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0523 03:08:40.548762   10489 ssh_runner.go:362] scp /home/ismailyasser/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0523 03:08:40.580246   10489 ssh_runner.go:362] scp /home/ismailyasser/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0523 03:08:40.610817   10489 ssh_runner.go:362] scp /home/ismailyasser/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0523 03:08:40.640970   10489 ssh_runner.go:362] scp /home/ismailyasser/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0523 03:08:40.673115   10489 ssh_runner.go:362] scp /home/ismailyasser/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0523 03:08:40.709371   10489 ssh_runner.go:362] scp /home/ismailyasser/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0523 03:08:40.744711   10489 ssh_runner.go:362] scp /home/ismailyasser/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0523 03:08:40.779347   10489 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I0523 03:08:40.807821   10489 ssh_runner.go:195] Run: openssl version
I0523 03:08:40.822718   10489 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0523 03:08:40.837927   10489 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0523 03:08:40.843551   10489 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Feb 20 18:40 /usr/share/ca-certificates/minikubeCA.pem
I0523 03:08:40.843612   10489 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0523 03:08:40.853432   10489 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0523 03:08:40.868742   10489 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0523 03:08:40.875624   10489 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0523 03:08:40.886299   10489 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0523 03:08:40.901207   10489 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0523 03:08:40.916881   10489 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0523 03:08:40.929406   10489 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0523 03:08:40.945353   10489 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0523 03:08:40.958730   10489 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2400 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/ismailyasser:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0523 03:08:40.958911   10489 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0523 03:08:41.034318   10489 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0523 03:08:41.058970   10489 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0523 03:08:41.058985   10489 kubeadm.go:593] restartPrimaryControlPlane start ...
I0523 03:08:41.059051   10489 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0523 03:08:41.084074   10489 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0523 03:08:41.084188   10489 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0523 03:08:41.572208   10489 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:32771"
I0523 03:08:41.645407   10489 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0523 03:08:41.695414   10489 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I0523 03:08:41.695450   10489 kubeadm.go:597] duration metric: took 636.458293ms to restartPrimaryControlPlane
I0523 03:08:41.695463   10489 kubeadm.go:394] duration metric: took 736.744492ms to StartCluster
I0523 03:08:41.695485   10489 settings.go:142] acquiring lock: {Name:mkcef2c3513b5ee636308e77b9f44033cbca33c5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0523 03:08:41.695600   10489 settings.go:150] Updating kubeconfig:  /home/ismailyasser/.kube/config
I0523 03:08:41.696745   10489 lock.go:35] WriteFile acquiring /home/ismailyasser/.kube/config: {Name:mkd88791e4cd94fcbd627dbd1e4d037845e3a3b4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0523 03:08:41.698460   10489 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0523 03:08:41.698575   10489 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0523 03:08:41.698640   10489 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0523 03:08:41.698789   10489 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0523 03:08:41.698814   10489 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0523 03:08:41.698824   10489 addons.go:247] addon storage-provisioner should already be in state true
I0523 03:08:41.698856   10489 host.go:66] Checking if "minikube" exists ...
I0523 03:08:41.698985   10489 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0523 03:08:41.699017   10489 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0523 03:08:41.699524   10489 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0523 03:08:41.699623   10489 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0523 03:08:41.699751   10489 addons.go:69] Setting metrics-server=true in profile "minikube"
I0523 03:08:41.699775   10489 addons.go:238] Setting addon metrics-server=true in "minikube"
W0523 03:08:41.699785   10489 addons.go:247] addon metrics-server should already be in state true
I0523 03:08:41.699822   10489 host.go:66] Checking if "minikube" exists ...
I0523 03:08:41.700587   10489 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0523 03:08:41.700758   10489 addons.go:69] Setting dashboard=true in profile "minikube"
I0523 03:08:41.700781   10489 addons.go:238] Setting addon dashboard=true in "minikube"
W0523 03:08:41.700791   10489 addons.go:247] addon dashboard should already be in state true
I0523 03:08:41.700829   10489 host.go:66] Checking if "minikube" exists ...
I0523 03:08:41.701556   10489 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0523 03:08:41.729524   10489 out.go:177] üîé  Verifying Kubernetes components...
I0523 03:08:41.750776   10489 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0523 03:08:41.970870   10489 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0523 03:08:41.994902   10489 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0523 03:08:42.071113   10489 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0523 03:08:42.071128   10489 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/dashboard:v2.7.0
I0523 03:08:42.079706   10489 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0523 03:08:42.079727   10489 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0523 03:08:42.079835   10489 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 03:08:42.082799   10489 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0523 03:08:42.088940   10489 addons.go:435] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0523 03:08:42.088964   10489 ssh_runner.go:362] scp dashboard/dashboard-ns.yaml --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0523 03:08:42.089060   10489 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 03:08:42.185819   10489 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0523 03:08:42.185843   10489 addons.go:247] addon default-storageclass should already be in state true
I0523 03:08:42.185888   10489 host.go:66] Checking if "minikube" exists ...
I0523 03:08:42.186611   10489 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0523 03:08:42.201855   10489 out.go:177]     ‚ñ™ Using image registry.k8s.io/metrics-server/metrics-server:v0.7.2
I0523 03:08:42.214014   10489 addons.go:435] installing /etc/kubernetes/addons/metrics-apiservice.yaml
I0523 03:08:42.214038   10489 ssh_runner.go:362] scp metrics-server/metrics-apiservice.yaml --> /etc/kubernetes/addons/metrics-apiservice.yaml (424 bytes)
I0523 03:08:42.214196   10489 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 03:08:42.633698   10489 api_server.go:52] waiting for apiserver process to appear ...
I0523 03:08:42.633824   10489 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 03:08:42.646506   10489 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/ismailyasser/.minikube/machines/minikube/id_rsa Username:docker}
I0523 03:08:42.773760   10489 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/ismailyasser/.minikube/machines/minikube/id_rsa Username:docker}
I0523 03:08:42.774385   10489 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/ismailyasser/.minikube/machines/minikube/id_rsa Username:docker}
I0523 03:08:42.816387   10489 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0523 03:08:42.816407   10489 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0523 03:08:42.816494   10489 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 03:08:43.134797   10489 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 03:08:43.183887   10489 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0523 03:08:43.183907   10489 ssh_runner.go:362] scp dashboard/dashboard-clusterrole.yaml --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0523 03:08:43.197335   10489 addons.go:435] installing /etc/kubernetes/addons/metrics-server-deployment.yaml
I0523 03:08:43.197354   10489 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-deployment.yaml (1907 bytes)
I0523 03:08:43.201085   10489 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0523 03:08:43.303474   10489 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0523 03:08:43.303494   10489 ssh_runner.go:362] scp dashboard/dashboard-clusterrolebinding.yaml --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0523 03:08:43.321809   10489 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/ismailyasser/.minikube/machines/minikube/id_rsa Username:docker}
I0523 03:08:43.322639   10489 addons.go:435] installing /etc/kubernetes/addons/metrics-server-rbac.yaml
I0523 03:08:43.322653   10489 ssh_runner.go:362] scp metrics-server/metrics-server-rbac.yaml --> /etc/kubernetes/addons/metrics-server-rbac.yaml (2175 bytes)
I0523 03:08:43.495280   10489 addons.go:435] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0523 03:08:43.495300   10489 ssh_runner.go:362] scp dashboard/dashboard-configmap.yaml --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0523 03:08:43.498716   10489 addons.go:435] installing /etc/kubernetes/addons/metrics-server-service.yaml
I0523 03:08:43.498735   10489 ssh_runner.go:362] scp metrics-server/metrics-server-service.yaml --> /etc/kubernetes/addons/metrics-server-service.yaml (446 bytes)
I0523 03:08:43.633952   10489 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 03:08:43.638461   10489 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0523 03:08:43.638912   10489 addons.go:435] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0523 03:08:43.638932   10489 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0523 03:08:43.804729   10489 addons.go:435] installing /etc/kubernetes/addons/dashboard-role.yaml
I0523 03:08:43.804750   10489 ssh_runner.go:362] scp dashboard/dashboard-role.yaml --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0523 03:08:43.819874   10489 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0523 03:08:43.890073   10489 addons.go:435] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0523 03:08:43.890096   10489 ssh_runner.go:362] scp dashboard/dashboard-rolebinding.yaml --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0523 03:08:44.096522   10489 addons.go:435] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0523 03:08:44.096541   10489 ssh_runner.go:362] scp dashboard/dashboard-sa.yaml --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0523 03:08:44.206481   10489 addons.go:435] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0523 03:08:44.206503   10489 ssh_runner.go:362] scp dashboard/dashboard-secret.yaml --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0523 03:08:44.426178   10489 addons.go:435] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0523 03:08:44.426199   10489 ssh_runner.go:362] scp dashboard/dashboard-svc.yaml --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0523 03:08:44.611570   10489 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0523 03:08:45.006305   10489 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.805182174s)
W0523 03:08:45.006357   10489 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 03:08:45.006387   10489 retry.go:31] will retry after 213.325836ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 03:08:45.006437   10489 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.372463912s)
I0523 03:08:45.006982   10489 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 03:08:45.012109   10489 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (1.373610478s)
W0523 03:08:45.012177   10489 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 03:08:45.012200   10489 retry.go:31] will retry after 361.834631ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 03:08:45.012308   10489 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.192361924s)
W0523 03:08:45.012373   10489 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 03:08:45.012408   10489 retry.go:31] will retry after 216.128458ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0523 03:08:45.184061   10489 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 03:08:45.184095   10489 retry.go:31] will retry after 202.791709ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 03:08:45.184170   10489 api_server.go:72] duration metric: took 3.485660881s to wait for apiserver process to appear ...
I0523 03:08:45.184205   10489 api_server.go:88] waiting for apiserver healthz status ...
I0523 03:08:45.184230   10489 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0523 03:08:45.194111   10489 api_server.go:269] stopped: https://127.0.0.1:32771/healthz: Get "https://127.0.0.1:32771/healthz": read tcp 127.0.0.1:43798->127.0.0.1:32771: read: connection reset by peer
I0523 03:08:45.226088   10489 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0523 03:08:45.229276   10489 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0523 03:08:45.374865   10489 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0523 03:08:45.387834   10489 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0523 03:08:45.685285   10489 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0523 03:08:45.705651   10489 api_server.go:269] stopped: https://127.0.0.1:32771/healthz: Get "https://127.0.0.1:32771/healthz": EOF
W0523 03:08:45.826334   10489 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 03:08:45.826370   10489 retry.go:31] will retry after 349.909306ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0523 03:08:45.829857   10489 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 03:08:45.829920   10489 retry.go:31] will retry after 402.935205ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0523 03:08:45.994882   10489 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 03:08:45.994931   10489 retry.go:31] will retry after 231.584075ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0523 03:08:45.995412   10489 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 03:08:45.995463   10489 retry.go:31] will retry after 262.355618ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 03:08:46.177936   10489 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0523 03:08:46.186120   10489 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0523 03:08:46.187304   10489 api_server.go:269] stopped: https://127.0.0.1:32771/healthz: Get "https://127.0.0.1:32771/healthz": EOF
I0523 03:08:46.228529   10489 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0523 03:08:46.234137   10489 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0523 03:08:46.264650   10489 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W0523 03:08:46.606594   10489 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 03:08:46.606624   10489 retry.go:31] will retry after 823.696879ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 03:08:46.684644   10489 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0523 03:08:46.685824   10489 api_server.go:269] stopped: https://127.0.0.1:32771/healthz: Get "https://127.0.0.1:32771/healthz": EOF
W0523 03:08:46.704991   10489 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 03:08:46.705021   10489 retry.go:31] will retry after 778.100507ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0523 03:08:46.705115   10489 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 03:08:46.705135   10489 retry.go:31] will retry after 612.379999ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0523 03:08:46.706365   10489 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 03:08:46.706392   10489 retry.go:31] will retry after 392.399767ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 03:08:47.099088   10489 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0523 03:08:47.185052   10489 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0523 03:08:47.186568   10489 api_server.go:269] stopped: https://127.0.0.1:32771/healthz: Get "https://127.0.0.1:32771/healthz": EOF
I0523 03:08:47.318110   10489 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0523 03:08:47.431558   10489 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0523 03:08:47.483686   10489 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
W0523 03:08:47.609796   10489 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 03:08:47.609857   10489 retry.go:31] will retry after 722.495068ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 03:08:47.685036   10489 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0523 03:08:47.685905   10489 api_server.go:269] stopped: https://127.0.0.1:32771/healthz: Get "https://127.0.0.1:32771/healthz": EOF
W0523 03:08:47.801310   10489 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 03:08:47.801341   10489 retry.go:31] will retry after 994.643901ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 03:08:48.184742   10489 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0523 03:08:48.333540   10489 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0523 03:08:48.796931   10489 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0523 03:08:51.437084   10489 api_server.go:279] https://127.0.0.1:32771/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0523 03:08:51.437103   10489 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0523 03:08:51.437115   10489 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0523 03:08:51.446811   10489 api_server.go:279] https://127.0.0.1:32771/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0523 03:08:51.446854   10489 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0523 03:08:51.684570   10489 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0523 03:08:51.694852   10489 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0523 03:08:51.694872   10489 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0523 03:08:52.185199   10489 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0523 03:08:52.190668   10489 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0523 03:08:52.190702   10489 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0523 03:08:52.684301   10489 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0523 03:08:52.694070   10489 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0523 03:08:52.694099   10489 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0523 03:08:54.091004   10489 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0523 03:08:54.103546   10489 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0523 03:08:54.103572   10489 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0523 03:08:54.591265   10489 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0523 03:08:54.628645   10489 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0523 03:08:54.628672   10489 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0523 03:08:55.091060   10489 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0523 03:08:55.116970   10489 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0523 03:08:55.117004   10489 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0523 03:08:55.590892   10489 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0523 03:08:55.624534   10489 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0523 03:08:55.624560   10489 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0523 03:08:56.090501   10489 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0523 03:08:56.191094   10489 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0523 03:08:56.191186   10489 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0523 03:08:56.591181   10489 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0523 03:08:56.607717   10489 api_server.go:279] https://127.0.0.1:32771/healthz returned 200:
ok
I0523 03:08:56.701583   10489 api_server.go:141] control plane version: v1.32.0
I0523 03:08:56.701612   10489 api_server.go:131] duration metric: took 10.611193376s to wait for apiserver health ...
I0523 03:08:56.701622   10489 system_pods.go:43] waiting for kube-system pods to appear ...
I0523 03:08:56.722027   10489 system_pods.go:59] 8 kube-system pods found
I0523 03:08:56.722052   10489 system_pods.go:61] "coredns-668d6bf9bc-p5mbn" [7c0fbd0c-20e0-4f66-8a2d-76a9b00c9a69] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0523 03:08:56.722060   10489 system_pods.go:61] "etcd-minikube" [3191dd3e-3c25-4ae3-8b24-2c0db7553bed] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0523 03:08:56.722067   10489 system_pods.go:61] "kube-apiserver-minikube" [e29af4d5-7bfd-474f-84ad-cf0d3b37c411] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0523 03:08:56.722077   10489 system_pods.go:61] "kube-controller-manager-minikube" [4c9f0d52-ef96-4500-8b9d-1826b6f63a39] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0523 03:08:56.722082   10489 system_pods.go:61] "kube-proxy-gqfbc" [ba3148c8-a347-4cee-82cd-90d200ac07ab] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0523 03:08:56.722094   10489 system_pods.go:61] "kube-scheduler-minikube" [dba2900b-90a6-4d04-b00c-895b27e96001] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0523 03:08:56.722099   10489 system_pods.go:61] "metrics-server-7fbb699795-nlw78" [df2250cd-99e1-4529-83a8-7c853d5f7fd8] Running / Ready:ContainersNotReady (containers with unready status: [metrics-server]) / ContainersReady:ContainersNotReady (containers with unready status: [metrics-server])
I0523 03:08:56.722104   10489 system_pods.go:61] "storage-provisioner" [f0a13351-6b27-484e-9869-2d4b935f4416] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0523 03:08:56.722111   10489 system_pods.go:74] duration metric: took 20.482457ms to wait for pod list to return data ...
I0523 03:08:56.722124   10489 kubeadm.go:582] duration metric: took 14.117417198s to wait for: map[apiserver:true system_pods:true]
I0523 03:08:56.722136   10489 node_conditions.go:102] verifying NodePressure condition ...
I0523 03:08:56.818194   10489 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0523 03:08:56.818224   10489 node_conditions.go:123] node cpu capacity is 8
I0523 03:08:56.818240   10489 node_conditions.go:105] duration metric: took 96.09957ms to run NodePressure ...
I0523 03:08:56.818349   10489 start.go:241] waiting for startup goroutines ...
I0523 03:09:00.388890   10489 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (11.998931324s)
I0523 03:09:00.388923   10489 addons.go:479] Verifying addon metrics-server=true in "minikube"
I0523 03:09:00.388965   10489 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (12.051185539s)
I0523 03:09:06.906138   10489 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (17.202855175s)
I0523 03:09:06.906289   10489 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (17.666362254s)
I0523 03:09:06.916911   10489 out.go:177] üí°  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server

I0523 03:09:06.935372   10489 out.go:177] üåü  Enabled addons: metrics-server, default-storageclass, storage-provisioner, dashboard
I0523 03:09:06.940998   10489 addons.go:514] duration metric: took 24.336136402s for enable addons: enabled=[metrics-server default-storageclass storage-provisioner dashboard]
I0523 03:09:06.941078   10489 start.go:246] waiting for cluster config update ...
I0523 03:09:06.941101   10489 start.go:255] writing updated cluster config ...
I0523 03:09:06.941413   10489 ssh_runner.go:195] Run: rm -f paused
I0523 03:09:07.455196   10489 start.go:600] kubectl: 1.31.0, cluster: 1.32.0 (minor skew: 1)
I0523 03:09:07.465586   10489 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
May 23 00:08:57 minikube cri-dockerd[1483]: time="2025-05-23T00:08:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1e2095446b8ba990ba21b24ff8515586c76e6be300aa2db2ba6c236a0777cda1/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
May 23 00:08:58 minikube cri-dockerd[1483]: time="2025-05-23T00:08:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/147f281d385b1b26a944aec7c075352d6f95e38a99fd9266bac5ae3b10d08fa2/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 23 00:08:58 minikube cri-dockerd[1483]: time="2025-05-23T00:08:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/616760c045480097a0e4e1207c0e3e091ca3f1575fc1c7f0e65e5f69f2af2bdb/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
May 23 00:08:59 minikube cri-dockerd[1483]: time="2025-05-23T00:08:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0d7c142463af6be60a8d52c0517c41e064dd27fd3d5cb14dcf4408b3ca73f234/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 23 00:09:00 minikube cri-dockerd[1483]: time="2025-05-23T00:09:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/79b956afb835238dc86b00712f8201b8c6ba019f40d2c4c9d9ee7b5e4f17bddf/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 23 00:09:00 minikube cri-dockerd[1483]: time="2025-05-23T00:09:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8103266eb73b8976174f0b8263e63b9d9d4d205b37746cb60f99b31651d5d4fa/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 23 00:09:00 minikube cri-dockerd[1483]: time="2025-05-23T00:09:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/68fa2602374de3d3efad0247b60502dc00912da0e1c263fa8afa8112c212843b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 23 00:09:00 minikube cri-dockerd[1483]: time="2025-05-23T00:09:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e5c1c52fc6f2d4a5c2aeba3d4f450eef54f72fc224357626580d8d8db57bec29/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 23 00:09:00 minikube cri-dockerd[1483]: time="2025-05-23T00:09:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/138c3954ba29a4a272d22bbce047bdf952c8a095056249b05c2307164182f373/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 23 00:09:01 minikube cri-dockerd[1483]: time="2025-05-23T00:09:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3796c259292aa5ca1b3c40326c91e167da21b7c59c1dcc8e26665767e2977dbb/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 23 00:09:01 minikube cri-dockerd[1483]: time="2025-05-23T00:09:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a6fc75fc65af41fb091d3a99c067c51296751e9aa19274deda722d75d565aae6/resolv.conf as [nameserver 10.96.0.10 search kube-system.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 23 00:09:15 minikube cri-dockerd[1483]: time="2025-05-23T00:09:15Z" level=info msg="Pulling image ismaill370/help-service:latest: c6b30c3f1696: Downloading [=====================>                             ]  19.38MB/45.68MB"
May 23 00:09:27 minikube cri-dockerd[1483]: time="2025-05-23T00:09:27Z" level=info msg="Pulling image ismaill370/help-service:latest: e23f099911d6: Downloading [======>                                            ]  26.32MB/211.4MB"
May 23 00:09:37 minikube cri-dockerd[1483]: time="2025-05-23T00:09:37Z" level=info msg="Pulling image ismaill370/help-service:latest: 433c262a5ae0: Downloading [=====>                                             ]  1.478MB/14.32MB"
May 23 00:09:38 minikube dockerd[1164]: time="2025-05-23T00:09:38.436930778Z" level=info msg="ignoring event" container=748a57e2d21a2377f54739106bbefcc1b8778301ac11a17bc346fe18c0e1053b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 23 00:09:38 minikube dockerd[1164]: time="2025-05-23T00:09:38.635398817Z" level=info msg="ignoring event" container=f2deeac894ef2c6f38c0d7756b099039f3bd2b766a583bf7f0d0822ffeee2065 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 23 00:09:40 minikube dockerd[1164]: time="2025-05-23T00:09:40.986048451Z" level=info msg="ignoring event" container=97b03db8e471718d82685fd589a3fdb2e9245c04e6aa6ca4b1a099101aafb188 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 23 00:09:47 minikube cri-dockerd[1483]: time="2025-05-23T00:09:47Z" level=info msg="Pulling image ismaill370/help-service:latest: e23f099911d6: Downloading [===========>                                       ]  50.49MB/211.4MB"
May 23 00:10:00 minikube cri-dockerd[1483]: time="2025-05-23T00:10:00Z" level=info msg="Pulling image ismaill370/help-service:latest: e23f099911d6: Downloading [====================>                              ]  86.55MB/211.4MB"
May 23 00:10:10 minikube cri-dockerd[1483]: time="2025-05-23T00:10:10Z" level=info msg="Pulling image ismaill370/help-service:latest: e23f099911d6: Downloading [============================>                      ]  121.5MB/211.4MB"
May 23 00:10:20 minikube cri-dockerd[1483]: time="2025-05-23T00:10:20Z" level=info msg="Pulling image ismaill370/help-service:latest: e23f099911d6: Downloading [=====================================>             ]  159.7MB/211.4MB"
May 23 00:10:33 minikube cri-dockerd[1483]: time="2025-05-23T00:10:33Z" level=info msg="Pulling image ismaill370/help-service:latest: e23f099911d6: Downloading [==============================================>    ]  196.7MB/211.4MB"
May 23 00:10:43 minikube cri-dockerd[1483]: time="2025-05-23T00:10:43Z" level=info msg="Pulling image ismaill370/help-service:latest: e23f099911d6: Extracting [=========>                                         ]  38.44MB/211.4MB"
May 23 00:10:53 minikube cri-dockerd[1483]: time="2025-05-23T00:10:53Z" level=info msg="Pulling image ismaill370/help-service:latest: e23f099911d6: Extracting [=======================>                           ]  98.04MB/211.4MB"
May 23 00:11:04 minikube cri-dockerd[1483]: time="2025-05-23T00:11:04Z" level=info msg="Pulling image ismaill370/help-service:latest: e23f099911d6: Extracting [=================================================> ]  207.2MB/211.4MB"
May 23 00:11:14 minikube cri-dockerd[1483]: time="2025-05-23T00:11:14Z" level=info msg="Pulling image ismaill370/help-service:latest: c6b30c3f1696: Extracting [==============================================>    ]  42.66MB/45.68MB"
May 23 00:11:24 minikube cri-dockerd[1483]: time="2025-05-23T00:11:24Z" level=info msg="Pulling image ismaill370/help-service:latest: 433c262a5ae0: Extracting [=====>                                             ]  1.638MB/14.32MB"
May 23 00:11:27 minikube cri-dockerd[1483]: time="2025-05-23T00:11:27Z" level=info msg="Stop pulling image ismaill370/help-service:latest: Status: Downloaded newer image for ismaill370/help-service:latest"
May 23 00:11:43 minikube cri-dockerd[1483]: time="2025-05-23T00:11:43Z" level=info msg="Pulling image ismaill370/user-service:latest: 9f0e2458f20e: Extracting [========================>                          ]  2.097MB/4.253MB"
May 23 00:11:44 minikube cri-dockerd[1483]: time="2025-05-23T00:11:44Z" level=info msg="Stop pulling image ismaill370/user-service:latest: Status: Downloaded newer image for ismaill370/user-service:latest"
May 23 00:12:00 minikube cri-dockerd[1483]: time="2025-05-23T00:12:00Z" level=info msg="Pulling image ismaill370/frontend:latest: be8f6c348bac: Downloading [============>                                      ]  26.34MB/103MB"
May 23 00:12:13 minikube cri-dockerd[1483]: time="2025-05-23T00:12:13Z" level=info msg="Pulling image ismaill370/frontend:latest: be8f6c348bac: Downloading [=============================>                     ]  61.34MB/103MB"
May 23 00:12:23 minikube cri-dockerd[1483]: time="2025-05-23T00:12:23Z" level=info msg="Pulling image ismaill370/frontend:latest: be8f6c348bac: Downloading [=============================================>     ]  94.19MB/103MB"
May 23 00:12:33 minikube cri-dockerd[1483]: time="2025-05-23T00:12:33Z" level=info msg="Pulling image ismaill370/frontend:latest: be8f6c348bac: Extracting [==>                                                ]  5.014MB/103MB"
May 23 00:12:44 minikube cri-dockerd[1483]: time="2025-05-23T00:12:44Z" level=info msg="Pulling image ismaill370/frontend:latest: be8f6c348bac: Extracting [==>                                                ]  5.571MB/103MB"
May 23 00:12:54 minikube cri-dockerd[1483]: time="2025-05-23T00:12:54Z" level=info msg="Pulling image ismaill370/frontend:latest: be8f6c348bac: Extracting [==>                                                ]  6.128MB/103MB"
May 23 00:13:04 minikube cri-dockerd[1483]: time="2025-05-23T00:13:04Z" level=info msg="Pulling image ismaill370/frontend:latest: be8f6c348bac: Extracting [===>                                               ]  6.685MB/103MB"
May 23 00:13:15 minikube cri-dockerd[1483]: time="2025-05-23T00:13:15Z" level=info msg="Pulling image ismaill370/frontend:latest: be8f6c348bac: Extracting [=======>                                           ]   15.6MB/103MB"
May 23 00:13:25 minikube cri-dockerd[1483]: time="2025-05-23T00:13:25Z" level=info msg="Pulling image ismaill370/frontend:latest: be8f6c348bac: Extracting [==========>                                        ]  22.28MB/103MB"
May 23 00:13:35 minikube cri-dockerd[1483]: time="2025-05-23T00:13:35Z" level=info msg="Pulling image ismaill370/frontend:latest: be8f6c348bac: Extracting [===============>                                   ]  32.87MB/103MB"
May 23 00:13:47 minikube cri-dockerd[1483]: time="2025-05-23T00:13:47Z" level=info msg="Pulling image ismaill370/frontend:latest: be8f6c348bac: Extracting [=========================>                         ]  52.36MB/103MB"
May 23 00:13:57 minikube cri-dockerd[1483]: time="2025-05-23T00:13:57Z" level=info msg="Pulling image ismaill370/frontend:latest: 4f4fb700ef54: Pull complete "
May 23 00:14:06 minikube cri-dockerd[1483]: time="2025-05-23T00:14:06Z" level=info msg="Stop pulling image ismaill370/frontend:latest: Status: Downloaded newer image for ismaill370/frontend:latest"
May 23 00:13:21 minikube cri-dockerd[1483]: time="2025-05-23T00:13:21Z" level=info msg="Pulling image ismaill370/frontend:latest: c2e955d54fa4: Downloading [========================>                          ]  1.242MB/2.517MB"
May 23 00:14:32 minikube cri-dockerd[1483]: time="2025-05-23T00:14:32Z" level=info msg="Pulling image ismaill370/frontend:latest: 864b7ae09518: Downloading [=====================>                             ]   45.2MB/103MB"
May 23 00:14:42 minikube cri-dockerd[1483]: time="2025-05-23T00:14:42Z" level=info msg="Pulling image ismaill370/frontend:latest: 864b7ae09518: Downloading [===================================>               ]  73.18MB/103MB"
May 23 00:14:52 minikube cri-dockerd[1483]: time="2025-05-23T00:14:52Z" level=info msg="Pulling image ismaill370/frontend:latest: 864b7ae09518: Extracting [>                                                  ]  1.114MB/103MB"
May 23 00:15:02 minikube cri-dockerd[1483]: time="2025-05-23T00:15:02Z" level=info msg="Pulling image ismaill370/frontend:latest: 864b7ae09518: Extracting [==>                                                ]  5.014MB/103MB"
May 23 00:15:12 minikube cri-dockerd[1483]: time="2025-05-23T00:15:12Z" level=info msg="Pulling image ismaill370/frontend:latest: 864b7ae09518: Extracting [==>                                                ]  5.571MB/103MB"
May 23 00:15:22 minikube cri-dockerd[1483]: time="2025-05-23T00:15:22Z" level=info msg="Pulling image ismaill370/frontend:latest: 864b7ae09518: Extracting [==>                                                ]  6.128MB/103MB"
May 23 00:15:32 minikube cri-dockerd[1483]: time="2025-05-23T00:15:32Z" level=info msg="Pulling image ismaill370/frontend:latest: 864b7ae09518: Extracting [===>                                               ]  7.242MB/103MB"
May 23 00:15:42 minikube cri-dockerd[1483]: time="2025-05-23T00:15:42Z" level=info msg="Pulling image ismaill370/frontend:latest: 864b7ae09518: Extracting [=======>                                           ]   15.6MB/103MB"
May 23 00:15:55 minikube cri-dockerd[1483]: time="2025-05-23T00:15:55Z" level=info msg="Pulling image ismaill370/frontend:latest: 864b7ae09518: Extracting [==========>                                        ]  20.61MB/103MB"
May 23 00:16:05 minikube cri-dockerd[1483]: time="2025-05-23T00:16:05Z" level=info msg="Pulling image ismaill370/frontend:latest: 864b7ae09518: Extracting [=============>                                     ]  28.41MB/103MB"
May 23 00:16:15 minikube cri-dockerd[1483]: time="2025-05-23T00:16:15Z" level=info msg="Pulling image ismaill370/frontend:latest: 864b7ae09518: Extracting [=========================>                         ]  52.92MB/103MB"
May 23 00:16:28 minikube cri-dockerd[1483]: time="2025-05-23T00:16:28Z" level=info msg="Pulling image ismaill370/frontend:latest: 864b7ae09518: Extracting [==================================================>]    103MB/103MB"
May 23 00:16:37 minikube cri-dockerd[1483]: time="2025-05-23T00:16:37Z" level=info msg="Stop pulling image ismaill370/frontend:latest: Status: Downloaded newer image for ismaill370/frontend:latest"
May 23 00:16:49 minikube cri-dockerd[1483]: time="2025-05-23T00:16:49Z" level=info msg="Pulling image ismaill370/help-service:latest: 513d18dbc0d5: Downloading [===========================>                       ]   5.17MB/9.488MB"
May 23 00:16:01 minikube cri-dockerd[1483]: time="2025-05-23T00:16:01Z" level=info msg="Pulling image ismaill370/help-service:latest: 513d18dbc0d5: Downloading [===========================>                       ]   5.17MB/9.488MB"
May 23 00:17:12 minikube cri-dockerd[1483]: time="2025-05-23T00:17:12Z" level=info msg="Pulling image ismaill370/help-service:latest: b872ed8b930c: Download complete "


==> container status <==
CONTAINER           IMAGE                                                                                             CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
e574940abea0d       ismaill370/frontend@sha256:c51e64f52c952be6e453278547a48c99b1eb9739c82a23444bf97dccbf23fa67       41 seconds ago      Running             frontend                    0                   8103266eb73b8       frontend-deployment-6588ccddd-rkrqk
b860d138a4bd2       ismaill370/frontend@sha256:8768fdba4a1bbc0bac5cf1d94e678c9590fe6127c7a78ad0d47a9a861906f73b       3 minutes ago       Running             frontend                    0                   68fa2602374de       frontend-deployment-6588ccddd-rjmgr
fa23992c506c8       ismaill370/user-service@sha256:268f233c8c12a4024e220ceb2f7d4a23dc99dfa000ac36c0050d722e29c833a4   5 minutes ago       Running             user-service                0                   0d7c142463af6       user-service-deployment-799db9c9f6-4kvt5
2ab17ac41b90d       ismaill370/help-service@sha256:d18a36ddbd2c95fbc8c7e648be742cda138594b4830aa3e89a6e8748272b3e4a   5 minutes ago       Running             help-service                0                   147f281d385b1       help-service-deployment-986dffd9d-8hbzs
a846cd77b3ec7       6e38f40d628db                                                                                     7 minutes ago       Running             storage-provisioner         20                  1e2095446b8ba       storage-provisioner
dd373b2ce65cd       48d9cfaaf3904                                                                                     7 minutes ago       Running             metrics-server              2                   a6fc75fc65af4       metrics-server-7fbb699795-nlw78
d281423bcef90       07655ddf2eebe                                                                                     7 minutes ago       Running             kubernetes-dashboard        16                  138c3954ba29a       kubernetes-dashboard-7779f9b69b-9lfsr
748a57e2d21a2       6e38f40d628db                                                                                     8 minutes ago       Exited              storage-provisioner         19                  1e2095446b8ba       storage-provisioner
35187730bef25       040f9f8aac8cd                                                                                     8 minutes ago       Running             kube-proxy                  10                  5e334600f39b0       kube-proxy-gqfbc
97b03db8e4717       48d9cfaaf3904                                                                                     8 minutes ago       Exited              metrics-server              1                   a6fc75fc65af4       metrics-server-7fbb699795-nlw78
f2deeac894ef2       07655ddf2eebe                                                                                     8 minutes ago       Exited              kubernetes-dashboard        15                  138c3954ba29a       kubernetes-dashboard-7779f9b69b-9lfsr
5151d8a2e736c       115053965e86b                                                                                     8 minutes ago       Running             dashboard-metrics-scraper   9                   79b956afb8352       dashboard-metrics-scraper-5d59dccf9b-h28ks
f3414fa38afa0       c69fa2e9cbf5f                                                                                     8 minutes ago       Running             coredns                     10                  616760c045480       coredns-668d6bf9bc-p5mbn
ee75983780239       c2e17b8d0f4a3                                                                                     8 minutes ago       Running             kube-apiserver              10                  a9fe036357975       kube-apiserver-minikube
8e6a41186c5ce       a9e7e6b294baf                                                                                     8 minutes ago       Running             etcd                        10                  839e4bf0d056a       etcd-minikube
24684a8a1aaa8       8cab3d2a8bd0f                                                                                     8 minutes ago       Running             kube-controller-manager     10                  b40758b7c3bf0       kube-controller-manager-minikube
21066fc74a3f7       a389e107f4ff1                                                                                     8 minutes ago       Running             kube-scheduler              10                  28b6597f8ceb3       kube-scheduler-minikube
c60030d249ef9       c69fa2e9cbf5f                                                                                     36 minutes ago      Exited              coredns                     9                   183aa57ac9312       coredns-668d6bf9bc-p5mbn
a80199c7db12f       115053965e86b                                                                                     36 minutes ago      Exited              dashboard-metrics-scraper   8                   50cbe57383ac0       dashboard-metrics-scraper-5d59dccf9b-h28ks
63984bbaf47f2       040f9f8aac8cd                                                                                     36 minutes ago      Exited              kube-proxy                  9                   e120a4090dc43       kube-proxy-gqfbc
3c6724f18973e       8cab3d2a8bd0f                                                                                     36 minutes ago      Exited              kube-controller-manager     9                   6957c396f8fcb       kube-controller-manager-minikube
10e77022a3895       a389e107f4ff1                                                                                     36 minutes ago      Exited              kube-scheduler              9                   9f309a06bb62c       kube-scheduler-minikube
3d901800b3f9e       c2e17b8d0f4a3                                                                                     36 minutes ago      Exited              kube-apiserver              9                   ecb5751dbd03f       kube-apiserver-minikube
11412591a6fc7       a9e7e6b294baf                                                                                     36 minutes ago      Exited              etcd                        9                   12b84e2549419       etcd-minikube


==> coredns [c60030d249ef] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:46931 - 21676 "HINFO IN 4293606702891014904.8506304783717038827. udp 57 false 512" - - 0 6.006991707s
[ERROR] plugin/errors: 2 4293606702891014904.8506304783717038827. HINFO: read udp 10.244.0.86:51703->192.168.1.7:53: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] 127.0.0.1:48128 - 15839 "HINFO IN 4293606702891014904.8506304783717038827. udp 57 false 512" - - 0 6.003205508s
[ERROR] plugin/errors: 2 4293606702891014904.8506304783717038827. HINFO: read udp 10.244.0.86:52554->192.168.1.7:53: i/o timeout
[INFO] 127.0.0.1:41371 - 1488 "HINFO IN 4293606702891014904.8506304783717038827. udp 57 false 512" - - 0 4.002547636s
[ERROR] plugin/errors: 2 4293606702891014904.8506304783717038827. HINFO: read udp 10.244.0.86:47157->192.168.1.7:53: i/o timeout
[INFO] 127.0.0.1:36458 - 25823 "HINFO IN 4293606702891014904.8506304783717038827. udp 57 false 512" - - 0 2.000615065s
[ERROR] plugin/errors: 2 4293606702891014904.8506304783717038827. HINFO: read udp 10.244.0.86:59558->192.168.1.7:53: i/o timeout
[INFO] 127.0.0.1:46375 - 49540 "HINFO IN 4293606702891014904.8506304783717038827. udp 57 false 512" - - 0 2.000881265s
[ERROR] plugin/errors: 2 4293606702891014904.8506304783717038827. HINFO: read udp 10.244.0.86:55055->192.168.1.7:53: i/o timeout
[INFO] 127.0.0.1:52850 - 38293 "HINFO IN 4293606702891014904.8506304783717038827. udp 57 false 512" - - 0 2.00079229s
[ERROR] plugin/errors: 2 4293606702891014904.8506304783717038827. HINFO: read udp 10.244.0.86:41967->192.168.1.7:53: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] 127.0.0.1:52346 - 26012 "HINFO IN 4293606702891014904.8506304783717038827. udp 57 false 512" - - 0 2.000984451s
[ERROR] plugin/errors: 2 4293606702891014904.8506304783717038827. HINFO: read udp 10.244.0.86:34565->192.168.1.7:53: i/o timeout
[INFO] 127.0.0.1:60117 - 57679 "HINFO IN 4293606702891014904.8506304783717038827. udp 57 false 512" - - 0 2.000658864s
[ERROR] plugin/errors: 2 4293606702891014904.8506304783717038827. HINFO: read udp 10.244.0.86:54706->192.168.1.7:53: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1207238862]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (22-May-2025 23:41:00.731) (total time: 30006ms):
Trace[1207238862]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30005ms (23:41:31.627)
Trace[1207238862]: [30.006079415s] [30.006079415s] END
[INFO] plugin/kubernetes: Trace[1470362609]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (22-May-2025 23:41:00.732) (total time: 30005ms):
Trace[1470362609]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30005ms (23:41:31.627)
Trace[1470362609]: [30.005917415s] [30.005917415s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[130408535]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (22-May-2025 23:41:00.732) (total time: 30005ms):
Trace[130408535]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30005ms (23:41:31.627)
Trace[130408535]: [30.005530215s] [30.005530215s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] 127.0.0.1:51329 - 33840 "HINFO IN 4293606702891014904.8506304783717038827. udp 57 false 512" - - 0 2.001176864s
[ERROR] plugin/errors: 2 4293606702891014904.8506304783717038827. HINFO: read udp 10.244.0.86:40263->192.168.1.7:53: i/o timeout
[INFO] 127.0.0.1:41801 - 6047 "HINFO IN 4293606702891014904.8506304783717038827. udp 57 false 512" - - 0 2.000926365s
[ERROR] plugin/errors: 2 4293606702891014904.8506304783717038827. HINFO: read udp 10.244.0.86:34827->192.168.1.7:53: i/o timeout
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [f3414fa38afa] <==
[INFO] 10.244.0.91:51204 - 50847 "AAAA IN ac-cbphxfe-shard-00-02.bwzvhcr.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000358783s
[INFO] 10.244.0.91:51204 - 27537 "A IN ac-cbphxfe-shard-00-02.bwzvhcr.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000425425s
[INFO] 10.244.0.91:48783 - 48327 "AAAA IN ac-cbphxfe-shard-00-02.bwzvhcr.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 390 0.000331559s
[INFO] 10.244.0.91:48783 - 52475 "A IN ac-cbphxfe-shard-00-02.bwzvhcr.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 342 0.0003806s
[INFO] 10.244.0.91:54333 - 23470 "A IN ac-cbphxfe-shard-00-00.bwzvhcr.mongodb.net.default.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000287558s
[INFO] 10.244.0.91:54333 - 12202 "AAAA IN ac-cbphxfe-shard-00-00.bwzvhcr.mongodb.net.default.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.0004664s
[INFO] 10.244.0.91:40762 - 23410 "AAAA IN ac-cbphxfe-shard-00-00.bwzvhcr.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.000312492s
[INFO] 10.244.0.91:40762 - 3967 "A IN ac-cbphxfe-shard-00-00.bwzvhcr.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.000277383s
[INFO] 10.244.0.91:43246 - 35170 "AAAA IN ac-cbphxfe-shard-00-00.bwzvhcr.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000153358s
[INFO] 10.244.0.91:43246 - 58750 "A IN ac-cbphxfe-shard-00-00.bwzvhcr.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000223942s
[INFO] 10.244.0.91:36729 - 37597 "AAAA IN ac-cbphxfe-shard-00-00.bwzvhcr.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 390 0.000193691s
[INFO] 10.244.0.91:36729 - 50145 "A IN ac-cbphxfe-shard-00-00.bwzvhcr.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 342 0.000273075s
[INFO] 10.244.0.91:36494 - 31441 "AAAA IN ac-cbphxfe-shard-00-01.bwzvhcr.mongodb.net.default.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000407275s
[INFO] 10.244.0.91:36494 - 27433 "A IN ac-cbphxfe-shard-00-01.bwzvhcr.mongodb.net.default.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000615725s
[INFO] 10.244.0.91:52892 - 39651 "AAAA IN ac-cbphxfe-shard-00-01.bwzvhcr.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.000334125s
[INFO] 10.244.0.91:52892 - 6372 "A IN ac-cbphxfe-shard-00-01.bwzvhcr.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.0004147s
[INFO] 10.244.0.91:55496 - 56010 "AAAA IN ac-cbphxfe-shard-00-01.bwzvhcr.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000392883s
[INFO] 10.244.0.91:55496 - 23252 "A IN ac-cbphxfe-shard-00-01.bwzvhcr.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.00058685s
[INFO] 10.244.0.91:41451 - 20064 "AAAA IN ac-cbphxfe-shard-00-01.bwzvhcr.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 390 0.00033275s
[INFO] 10.244.0.91:41451 - 25446 "A IN ac-cbphxfe-shard-00-01.bwzvhcr.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 342 0.000443575s
[INFO] 10.244.0.91:58873 - 43936 "AAAA IN ac-cbphxfe-shard-00-02.bwzvhcr.mongodb.net.default.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000494084s
[INFO] 10.244.0.91:58873 - 58009 "A IN ac-cbphxfe-shard-00-02.bwzvhcr.mongodb.net.default.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000556875s
[INFO] 10.244.0.91:38137 - 6974 "AAAA IN ac-cbphxfe-shard-00-02.bwzvhcr.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.000534967s
[INFO] 10.244.0.91:38137 - 29238 "A IN ac-cbphxfe-shard-00-02.bwzvhcr.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.000642859s
[INFO] 10.244.0.91:55497 - 39917 "AAAA IN ac-cbphxfe-shard-00-02.bwzvhcr.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000671917s
[INFO] 10.244.0.91:55497 - 63972 "A IN ac-cbphxfe-shard-00-02.bwzvhcr.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000991008s
[INFO] 10.244.0.91:54597 - 6241 "A IN ac-cbphxfe-shard-00-02.bwzvhcr.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 342 0.000238883s
[INFO] 10.244.0.91:48924 - 12610 "AAAA IN ac-cbphxfe-shard-00-00.bwzvhcr.mongodb.net.default.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000352642s
[INFO] 10.244.0.91:54597 - 34169 "AAAA IN ac-cbphxfe-shard-00-02.bwzvhcr.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 390 0.00029645s
[INFO] 10.244.0.91:48924 - 62810 "A IN ac-cbphxfe-shard-00-00.bwzvhcr.mongodb.net.default.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.00049335s
[INFO] 10.244.0.91:46664 - 23119 "AAAA IN ac-cbphxfe-shard-00-00.bwzvhcr.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.000570167s
[INFO] 10.244.0.91:46664 - 10103 "A IN ac-cbphxfe-shard-00-00.bwzvhcr.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.000596842s
[INFO] 10.244.0.91:51085 - 27038 "AAAA IN ac-cbphxfe-shard-00-00.bwzvhcr.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000514342s
[INFO] 10.244.0.91:51085 - 5222 "A IN ac-cbphxfe-shard-00-00.bwzvhcr.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000841499s
[INFO] 10.244.0.91:35097 - 56683 "AAAA IN ac-cbphxfe-shard-00-00.bwzvhcr.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 390 0.000193509s
[INFO] 10.244.0.91:35097 - 60769 "A IN ac-cbphxfe-shard-00-00.bwzvhcr.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 342 0.0003564s
[INFO] 10.244.0.91:59945 - 734 "A IN ac-cbphxfe-shard-00-01.bwzvhcr.mongodb.net.default.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000512875s
[INFO] 10.244.0.91:59945 - 62916 "AAAA IN ac-cbphxfe-shard-00-01.bwzvhcr.mongodb.net.default.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.001079741s
[INFO] 10.244.0.91:47675 - 27541 "AAAA IN ac-cbphxfe-shard-00-01.bwzvhcr.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.000487116s
[INFO] 10.244.0.91:47675 - 62860 "A IN ac-cbphxfe-shard-00-01.bwzvhcr.mongodb.net.svc.cluster.local. udp 78 false 512" NXDOMAIN qr,aa,rd 171 0.000754783s
[INFO] 10.244.0.91:32891 - 31269 "A IN ac-cbphxfe-shard-00-01.bwzvhcr.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000430375s
[INFO] 10.244.0.91:32891 - 31260 "AAAA IN ac-cbphxfe-shard-00-01.bwzvhcr.mongodb.net.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000725908s
[INFO] 10.244.0.91:38215 - 34216 "AAAA IN ac-cbphxfe-shard-00-01.bwzvhcr.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 390 0.0004015s
[INFO] 10.244.0.91:38215 - 43183 "A IN ac-cbphxfe-shard-00-01.bwzvhcr.mongodb.net. udp 60 false 512" NOERROR qr,aa,rd,ra 342 0.000439633s
[INFO] 10.244.0.93:44746 - 13650 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000412867s
[INFO] 10.244.0.93:44746 - 13284 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000550366s
[INFO] 10.244.0.93:50952 - 23100 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000258225s
[INFO] 10.244.0.93:50952 - 23558 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000422033s
[INFO] 10.244.0.93:48677 - 4485 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000178384s
[INFO] 10.244.0.93:48677 - 4119 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000217983s
[INFO] 10.244.0.93:37444 - 17006 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 444 0.151847229s
[INFO] 10.244.0.93:37444 - 17373 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 588 0.157155919s
[INFO] 10.244.0.94:59611 - 23718 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0004829s
[INFO] 10.244.0.94:59611 - 23260 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0007337s
[INFO] 10.244.0.94:38821 - 56844 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000280683s
[INFO] 10.244.0.94:38821 - 56294 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000462092s
[INFO] 10.244.0.94:36444 - 5604 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000219266s
[INFO] 10.244.0.94:36444 - 5238 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000185075s
[INFO] 10.244.0.94:38676 - 43536 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 444 0.074498598s
[INFO] 10.244.0.94:38676 - 43903 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 588 0.074974898s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_02_20T20_40_12_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 20 Feb 2025 18:40:10 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 23 May 2025 00:17:10 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 23 May 2025 00:16:42 +0000   Wed, 21 May 2025 17:22:51 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 23 May 2025 00:16:42 +0000   Wed, 21 May 2025 17:22:51 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 23 May 2025 00:16:42 +0000   Wed, 21 May 2025 17:22:51 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 23 May 2025 00:16:42 +0000   Wed, 21 May 2025 17:22:51 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             10102804Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             10102804Ki
  pods:               110
System Info:
  Machine ID:                 58bcae9e02b943ce9084ebfb3914b087
  System UUID:                58bcae9e02b943ce9084ebfb3914b087
  Boot ID:                    3916296b-3fa3-4465-9d14-74d07d0c522b
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (16 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     frontend-deployment-6588ccddd-rjmgr           250m (3%)     500m (6%)   256Mi (2%)       512Mi (5%)     49m
  default                     frontend-deployment-6588ccddd-rkrqk           250m (3%)     500m (6%)   256Mi (2%)       512Mi (5%)     49m
  default                     help-service-deployment-986dffd9d-8hbzs       0 (0%)        0 (0%)      0 (0%)           0 (0%)         49m
  default                     help-service-deployment-986dffd9d-kslrs       0 (0%)        0 (0%)      0 (0%)           0 (0%)         49m
  default                     user-service-deployment-799db9c9f6-4kvt5      0 (0%)        0 (0%)      0 (0%)           0 (0%)         49m
  default                     user-service-deployment-799db9c9f6-cj7kj      0 (0%)        0 (0%)      0 (0%)           0 (0%)         49m
  kube-system                 coredns-668d6bf9bc-p5mbn                      100m (1%)     0 (0%)      70Mi (0%)        170Mi (1%)     91d
  kube-system                 etcd-minikube                                 100m (1%)     0 (0%)      100Mi (1%)       0 (0%)         91d
  kube-system                 kube-apiserver-minikube                       250m (3%)     0 (0%)      0 (0%)           0 (0%)         91d
  kube-system                 kube-controller-manager-minikube              200m (2%)     0 (0%)      0 (0%)           0 (0%)         91d
  kube-system                 kube-proxy-gqfbc                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         91d
  kube-system                 kube-scheduler-minikube                       100m (1%)     0 (0%)      0 (0%)           0 (0%)         91d
  kube-system                 metrics-server-7fbb699795-nlw78               100m (1%)     0 (0%)      200Mi (2%)       0 (0%)         35m
  kube-system                 storage-provisioner                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         91d
  kubernetes-dashboard        dashboard-metrics-scraper-5d59dccf9b-h28ks    0 (0%)        0 (0%)      0 (0%)           0 (0%)         91d
  kubernetes-dashboard        kubernetes-dashboard-7779f9b69b-9lfsr         0 (0%)        0 (0%)      0 (0%)           0 (0%)         91d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1350m (16%)  1 (12%)
  memory             882Mi (8%)   1194Mi (12%)
  ephemeral-storage  0 (0%)       0 (0%)
  hugepages-1Gi      0 (0%)       0 (0%)
  hugepages-2Mi      0 (0%)       0 (0%)
Events:
  Type     Reason                             Age                    From             Message
  ----     ------                             ----                   ----             -------
  Normal   Starting                           30h                    kube-proxy       
  Normal   Starting                           50m                    kube-proxy       
  Normal   Starting                           8m10s                  kube-proxy       
  Normal   Starting                           45m                    kube-proxy       
  Normal   Starting                           36m                    kube-proxy       
  Normal   NodeNotReady                       30h                    kubelet          Node minikube status is now: NodeNotReady
  Normal   RegisteredNode                     30h                    node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   NodeHasSufficientMemory            30h (x24 over 32h)     kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeReady                          30h                    kubelet          Node minikube status is now: NodeReady
  Normal   NodeHasNoDiskPressure              30h (x24 over 32h)     kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               30h (x24 over 32h)     kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeNotReady                       30h                    node-controller  Node minikube status is now: NodeNotReady
  Warning  CgroupV1                           50m                    kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   Starting                           50m                    kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory            50m (x8 over 50m)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              50m (x8 over 50m)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               50m (x7 over 50m)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            50m                    kubelet          Updated Node Allocatable limit across pods
  Warning  PossibleMemoryBackedVolumesOnDisk  50m                    kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Warning  Rebooted                           50m                    kubelet          Node minikube has been rebooted, boot id: 3916296b-3fa3-4465-9d14-74d07d0c522b
  Normal   RegisteredNode                     50m                    node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   Starting                           45m                    kubelet          Starting kubelet.
  Warning  PossibleMemoryBackedVolumesOnDisk  45m                    kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Warning  CgroupV1                           45m                    kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeHasSufficientMemory            45m (x8 over 45m)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              45m (x8 over 45m)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               45m (x7 over 45m)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            45m                    kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode                     45m                    node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   NodeHasSufficientMemory            36m (x8 over 36m)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   Starting                           36m                    kubelet          Starting kubelet.
  Warning  CgroupV1                           36m                    kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Warning  PossibleMemoryBackedVolumesOnDisk  36m                    kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   NodeHasNoDiskPressure              36m (x8 over 36m)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               36m (x7 over 36m)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            36m                    kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode                     36m                    node-controller  Node minikube event: Registered Node minikube in Controller
  Warning  PossibleMemoryBackedVolumesOnDisk  8m37s                  kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           8m37s                  kubelet          Starting kubelet.
  Warning  CgroupV1                           8m37s                  kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeHasSufficientMemory            8m37s (x8 over 8m37s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              8m37s (x8 over 8m37s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               8m37s (x7 over 8m37s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            8m37s                  kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode                     8m16s                  node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[May22 23:40] Exception: 
[  +0.000010] Operation canceled @p9io.cpp:258 (AcceptAsync)

[May22 23:42] /sbin/ldconfig: 
[  +0.000006] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.034094] FS-Cache: Duplicate cookie detected
[  +0.000727] FS-Cache: O-cookie c=00000076 [p=00000002 fl=222 nc=0 na=1]
[  +0.000555] FS-Cache: O-cookie d=000000007c4caf4c{9P.session} n=00000000abc3d7de
[  +0.000741] FS-Cache: O-key=[10] '34323937373133383630'
[  +0.000455] FS-Cache: N-cookie c=00000077 [p=00000002 fl=2 nc=0 na=1]
[  +0.000600] FS-Cache: N-cookie d=000000007c4caf4c{9P.session} n=0000000051156a95
[  +0.000743] FS-Cache: N-key=[10] '34323937373133383630'
[  +0.127061] /sbin/ldconfig.real: 
[  +0.000007] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.082612] FS-Cache: Duplicate cookie detected
[  +0.000774] FS-Cache: O-cookie c=0000007c [p=00000002 fl=222 nc=0 na=1]
[  +0.000615] FS-Cache: O-cookie d=000000007c4caf4c{9P.session} n=000000002abecedb
[  +0.000805] FS-Cache: O-key=[10] '34323937373133383831'
[  +0.000484] FS-Cache: N-cookie c=0000007d [p=00000002 fl=2 nc=0 na=1]
[  +0.000606] FS-Cache: N-cookie d=000000007c4caf4c{9P.session} n=0000000051d100bc
[  +0.000776] FS-Cache: N-key=[10] '34323937373133383831'
[  +0.008452] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.002944] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000930] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000806] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001063] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002612] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001875] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001611] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001509] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001035] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.133382] new mount options do not match the existing superblock, will be ignored
[  +0.218470] Failed to connect to bus: No such file or directory
[  +0.259429] Failed to connect to bus: No such file or directory
[  +0.275027] Failed to connect to bus: No such file or directory
[  +0.060096] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.204274] Failed to connect to bus: No such file or directory
[  +0.314135] Failed to connect to bus: No such file or directory
[  +0.076109] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.050783] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001667] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.037380] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.024990] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.069365] Failed to connect to bus: No such file or directory
[  +0.040442] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000313] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000309] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000288] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.229193] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.058874] Failed to connect to bus: No such file or directory
[  +0.283650] Failed to connect to bus: No such file or directory
[  +0.270740] Failed to connect to bus: No such file or directory
[  +0.564274] systemd-journald[79]: File /var/log/journal/39afda6d5c4d4c1a8f205585c0184eaf/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +0.969081] WSL (2 - init-systemd(Ubuntu)) ERROR: WaitForBootProcess:3470: /sbin/init failed to start within 10000ms
[  +5.187489] systemd-journald[79]: File /var/log/journal/39afda6d5c4d4c1a8f205585c0184eaf/user-1000.journal corrupted or uncleanly shut down, renaming and replacing.
[May22 23:43] tmpfs: Unknown parameter 'noswap'
[May22 23:45] hrtimer: interrupt took 3429066 ns
[May23 00:12] tmpfs: Unknown parameter 'noswap'


==> etcd [11412591a6fc] <==
{"level":"info","ts":"2025-05-22T23:40:48.079253Z","caller":"traceutil/trace.go:171","msg":"trace[1598829423] transaction","detail":"{read_only:false; number_of_response:0; response_revision:20523; }","duration":"107.412906ms","start":"2025-05-22T23:40:47.971797Z","end":"2025-05-22T23:40:48.079210Z","steps":["trace[1598829423] 'process raft request'  (duration: 90.183394ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-22T23:40:48.079407Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.023787ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" limit:1 ","response":"range_response_count:1 size:423"}
{"level":"info","ts":"2025-05-22T23:40:48.079557Z","caller":"traceutil/trace.go:171","msg":"trace[1667597612] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:20523; }","duration":"101.304289ms","start":"2025-05-22T23:40:47.978223Z","end":"2025-05-22T23:40:48.079527Z","steps":["trace[1667597612] 'agreement among raft nodes before linearized reading'  (duration: 100.775918ms)"],"step_count":1}
{"level":"info","ts":"2025-05-22T23:40:48.278130Z","caller":"traceutil/trace.go:171","msg":"trace[1345079651] transaction","detail":"{read_only:false; number_of_response:0; response_revision:20524; }","duration":"101.811025ms","start":"2025-05-22T23:40:48.176277Z","end":"2025-05-22T23:40:48.278088Z","steps":["trace[1345079651] 'process raft request'  (duration: 87.099981ms)"],"step_count":1}
{"level":"info","ts":"2025-05-22T23:40:48.761404Z","caller":"traceutil/trace.go:171","msg":"trace[494272226] transaction","detail":"{read_only:false; response_revision:20525; number_of_response:1; }","duration":"174.598807ms","start":"2025-05-22T23:40:48.586786Z","end":"2025-05-22T23:40:48.761385Z","steps":["trace[494272226] 'process raft request'  (duration: 174.33563ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-22T23:40:50.295254Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"233.966978ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/system:controller:service-account-controller\" limit:1 ","response":"range_response_count:1 size:676"}
{"level":"info","ts":"2025-05-22T23:40:50.295370Z","caller":"traceutil/trace.go:171","msg":"trace[500053293] range","detail":"{range_begin:/registry/clusterroles/system:controller:service-account-controller; range_end:; response_count:1; response_revision:20555; }","duration":"234.148386ms","start":"2025-05-22T23:40:50.061178Z","end":"2025-05-22T23:40:50.295326Z","steps":["trace[500053293] 'range keys from in-memory index tree'  (duration: 233.809311ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-22T23:40:50.385232Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"217.793867ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128037429574316569 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/default/minikube.1841fdd8de511619\" mod_revision:20553 > success:<request_put:<key:\"/registry/events/default/minikube.1841fdd8de511619\" value_size:568 lease:8128037429574316447 >> failure:<request_range:<key:\"/registry/events/default/minikube.1841fdd8de511619\" > >>","response":"size:18"}
{"level":"info","ts":"2025-05-22T23:40:50.385334Z","caller":"traceutil/trace.go:171","msg":"trace[348137376] linearizableReadLoop","detail":"{readStateIndex:25221; appliedIndex:25220; }","duration":"206.519055ms","start":"2025-05-22T23:40:50.178797Z","end":"2025-05-22T23:40:50.385315Z","steps":["trace[348137376] 'read index received'  (duration: 33.642¬µs)","trace[348137376] 'applied index is now lower than readState.Index'  (duration: 206.483304ms)"],"step_count":2}
{"level":"info","ts":"2025-05-22T23:40:50.385389Z","caller":"traceutil/trace.go:171","msg":"trace[117500793] transaction","detail":"{read_only:false; response_revision:20556; number_of_response:1; }","duration":"298.075045ms","start":"2025-05-22T23:40:50.087255Z","end":"2025-05-22T23:40:50.385330Z","steps":["trace[117500793] 'process raft request'  (duration: 73.663214ms)","trace[117500793] 'compare'  (duration: 134.287307ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-22T23:40:50.385500Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"206.695513ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-05-22T23:40:50.385537Z","caller":"traceutil/trace.go:171","msg":"trace[1800322630] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:20556; }","duration":"206.766738ms","start":"2025-05-22T23:40:50.178759Z","end":"2025-05-22T23:40:50.385526Z","steps":["trace[1800322630] 'agreement among raft nodes before linearized reading'  (duration: 206.643905ms)"],"step_count":1}
{"level":"info","ts":"2025-05-22T23:40:50.787425Z","caller":"traceutil/trace.go:171","msg":"trace[691236635] linearizableReadLoop","detail":"{readStateIndex:25225; appliedIndex:25224; }","duration":"104.286835ms","start":"2025-05-22T23:40:50.683112Z","end":"2025-05-22T23:40:50.787399Z","steps":["trace[691236635] 'read index received'  (duration: 8.384655ms)","trace[691236635] 'applied index is now lower than readState.Index'  (duration: 95.900897ms)"],"step_count":2}
{"level":"info","ts":"2025-05-22T23:40:50.787502Z","caller":"traceutil/trace.go:171","msg":"trace[307711728] transaction","detail":"{read_only:false; response_revision:20560; number_of_response:1; }","duration":"108.222725ms","start":"2025-05-22T23:40:50.679229Z","end":"2025-05-22T23:40:50.787452Z","steps":["trace[307711728] 'process raft request'  (duration: 12.409737ms)","trace[307711728] 'compare'  (duration: 95.503522ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-22T23:40:50.787748Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.611976ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterrolebindings/cluster-admin\" limit:1 ","response":"range_response_count:1 size:680"}
{"level":"info","ts":"2025-05-22T23:40:50.787796Z","caller":"traceutil/trace.go:171","msg":"trace[385433421] range","detail":"{range_begin:/registry/clusterrolebindings/cluster-admin; range_end:; response_count:1; response_revision:20560; }","duration":"104.717302ms","start":"2025-05-22T23:40:50.683063Z","end":"2025-05-22T23:40:50.787780Z","steps":["trace[385433421] 'agreement among raft nodes before linearized reading'  (duration: 104.443677ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-22T23:40:52.290712Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"206.483488ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/roles/kube-system/system:controller:bootstrap-signer\" limit:1 ","response":"range_response_count:1 size:619"}
{"level":"info","ts":"2025-05-22T23:40:52.290850Z","caller":"traceutil/trace.go:171","msg":"trace[1159727449] range","detail":"{range_begin:/registry/roles/kube-system/system:controller:bootstrap-signer; range_end:; response_count:1; response_revision:20577; }","duration":"206.696429ms","start":"2025-05-22T23:40:52.084123Z","end":"2025-05-22T23:40:52.290819Z","steps":["trace[1159727449] 'range keys from in-memory index tree'  (duration: 206.38183ms)"],"step_count":1}
{"level":"info","ts":"2025-05-22T23:40:52.291275Z","caller":"traceutil/trace.go:171","msg":"trace[676013774] transaction","detail":"{read_only:false; response_revision:20578; number_of_response:1; }","duration":"116.175722ms","start":"2025-05-22T23:40:52.175079Z","end":"2025-05-22T23:40:52.291255Z","steps":["trace[676013774] 'process raft request'  (duration: 23.588208ms)","trace[676013774] 'compare'  (duration: 91.806056ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-22T23:40:52.561077Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"159.820314ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/roles/kube-system/system::leader-locking-kube-controller-manager\" limit:1 ","response":"range_response_count:1 size:828"}
{"level":"info","ts":"2025-05-22T23:40:52.561182Z","caller":"traceutil/trace.go:171","msg":"trace[1351386111] range","detail":"{range_begin:/registry/roles/kube-system/system::leader-locking-kube-controller-manager; range_end:; response_count:1; response_revision:20578; }","duration":"159.979905ms","start":"2025-05-22T23:40:52.401174Z","end":"2025-05-22T23:40:52.561154Z","steps":["trace[1351386111] 'range keys from in-memory index tree'  (duration: 159.702614ms)"],"step_count":1}
{"level":"info","ts":"2025-05-22T23:40:54.030155Z","caller":"traceutil/trace.go:171","msg":"trace[324437352] linearizableReadLoop","detail":"{readStateIndex:25245; appliedIndex:25244; }","duration":"270.972937ms","start":"2025-05-22T23:40:52.690357Z","end":"2025-05-22T23:40:54.030125Z","steps":["trace[324437352] 'read index received'  (duration: 270.677636ms)","trace[324437352] 'applied index is now lower than readState.Index'  (duration: 293.901¬µs)"],"step_count":2}
{"level":"warn","ts":"2025-05-22T23:40:54.030421Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"271.234596ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/roles/kube-public/system:controller:bootstrap-signer\" limit:1 ","response":"range_response_count:1 size:710"}
{"level":"info","ts":"2025-05-22T23:40:54.030485Z","caller":"traceutil/trace.go:171","msg":"trace[1296125150] range","detail":"{range_begin:/registry/roles/kube-public/system:controller:bootstrap-signer; range_end:; response_count:1; response_revision:20580; }","duration":"271.317996ms","start":"2025-05-22T23:40:52.690349Z","end":"2025-05-22T23:40:54.030462Z","steps":["trace[1296125150] 'agreement among raft nodes before linearized reading'  (duration: 271.143396ms)"],"step_count":1}
{"level":"info","ts":"2025-05-22T23:40:54.030808Z","caller":"traceutil/trace.go:171","msg":"trace[1335707068] transaction","detail":"{read_only:false; response_revision:20580; number_of_response:1; }","duration":"281.750061ms","start":"2025-05-22T23:40:52.680244Z","end":"2025-05-22T23:40:54.030789Z","steps":["trace[1335707068] 'process raft request'  (duration: 280.870157ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-22T23:40:54.330769Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"268.983708ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/rolebindings/kube-system/system::extension-apiserver-authentication-reader\" limit:1 ","response":"range_response_count:1 size:857"}
{"level":"info","ts":"2025-05-22T23:40:54.330978Z","caller":"traceutil/trace.go:171","msg":"trace[139509682] range","detail":"{range_begin:/registry/rolebindings/kube-system/system::extension-apiserver-authentication-reader; range_end:; response_count:1; response_revision:20580; }","duration":"269.290608ms","start":"2025-05-22T23:40:54.061657Z","end":"2025-05-22T23:40:54.330948Z","steps":["trace[139509682] 'range keys from in-memory index tree'  (duration: 268.835508ms)"],"step_count":1}
{"level":"info","ts":"2025-05-22T23:40:54.530102Z","caller":"traceutil/trace.go:171","msg":"trace[984402703] linearizableReadLoop","detail":"{readStateIndex:25246; appliedIndex:25245; }","duration":"190.503528ms","start":"2025-05-22T23:40:54.339568Z","end":"2025-05-22T23:40:54.530071Z","steps":["trace[984402703] 'read index received'  (duration: 190.208628ms)","trace[984402703] 'applied index is now lower than readState.Index'  (duration: 293.3¬µs)"],"step_count":2}
{"level":"info","ts":"2025-05-22T23:40:54.530286Z","caller":"traceutil/trace.go:171","msg":"trace[1822858099] transaction","detail":"{read_only:false; response_revision:20581; number_of_response:1; }","duration":"297.057787ms","start":"2025-05-22T23:40:54.233210Z","end":"2025-05-22T23:40:54.530268Z","steps":["trace[1822858099] 'process raft request'  (duration: 296.591087ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-22T23:40:54.530720Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"191.129928ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/rolebindings/kube-system/system::leader-locking-kube-controller-manager\" limit:1 ","response":"range_response_count:1 size:915"}
{"level":"info","ts":"2025-05-22T23:40:54.530774Z","caller":"traceutil/trace.go:171","msg":"trace[1545494251] range","detail":"{range_begin:/registry/rolebindings/kube-system/system::leader-locking-kube-controller-manager; range_end:; response_count:1; response_revision:20581; }","duration":"191.349727ms","start":"2025-05-22T23:40:54.339408Z","end":"2025-05-22T23:40:54.530758Z","steps":["trace[1545494251] 'agreement among raft nodes before linearized reading'  (duration: 191.191927ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-22T23:40:54.749833Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"377.887156ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kube-system/kube-controller-manager-minikube\" limit:1 ","response":"range_response_count:1 size:7366"}
{"level":"info","ts":"2025-05-22T23:40:54.749953Z","caller":"traceutil/trace.go:171","msg":"trace[1608757298] range","detail":"{range_begin:/registry/pods/kube-system/kube-controller-manager-minikube; range_end:; response_count:1; response_revision:20581; }","duration":"378.062556ms","start":"2025-05-22T23:40:54.371854Z","end":"2025-05-22T23:40:54.749916Z","steps":["trace[1608757298] 'agreement among raft nodes before linearized reading'  (duration: 159.162339ms)","trace[1608757298] 'range keys from in-memory index tree'  (duration: 218.603817ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-22T23:40:54.750021Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-22T23:40:54.371818Z","time spent":"378.182956ms","remote":"127.0.0.1:47996","response type":"/etcdserverpb.KV/Range","request count":0,"request size":63,"response count":1,"response size":7390,"request content":"key:\"/registry/pods/kube-system/kube-controller-manager-minikube\" limit:1 "}
{"level":"warn","ts":"2025-05-22T23:40:54.750562Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"214.849418ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/rolebindings/kube-system/system::leader-locking-kube-scheduler\" limit:1 ","response":"range_response_count:1 size:809"}
{"level":"info","ts":"2025-05-22T23:40:54.750620Z","caller":"traceutil/trace.go:171","msg":"trace[1447392882] range","detail":"{range_begin:/registry/rolebindings/kube-system/system::leader-locking-kube-scheduler; range_end:; response_count:1; response_revision:20581; }","duration":"214.961218ms","start":"2025-05-22T23:40:54.535639Z","end":"2025-05-22T23:40:54.750600Z","steps":["trace[1447392882] 'range keys from in-memory index tree'  (duration: 214.660018ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-22T23:40:54.750884Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"202.321423ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-05-22T23:40:54.750983Z","caller":"traceutil/trace.go:171","msg":"trace[429717418] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:20581; }","duration":"202.464722ms","start":"2025-05-22T23:40:54.548502Z","end":"2025-05-22T23:40:54.750967Z","steps":["trace[429717418] 'range keys from in-memory index tree'  (duration: 202.221723ms)"],"step_count":1}
{"level":"info","ts":"2025-05-22T23:40:54.751379Z","caller":"traceutil/trace.go:171","msg":"trace[205126147] transaction","detail":"{read_only:false; response_revision:20582; number_of_response:1; }","duration":"200.438123ms","start":"2025-05-22T23:40:54.550918Z","end":"2025-05-22T23:40:54.751356Z","steps":["trace[205126147] 'process raft request'  (duration: 108.414058ms)","trace[205126147] 'compare'  (duration: 90.153866ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-22T23:40:55.030742Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"166.860837ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/rolebindings/kube-system/system:controller:token-cleaner\" limit:1 ","response":"range_response_count:1 size:730"}
{"level":"info","ts":"2025-05-22T23:40:55.030836Z","caller":"traceutil/trace.go:171","msg":"trace[416367766] range","detail":"{range_begin:/registry/rolebindings/kube-system/system:controller:token-cleaner; range_end:; response_count:1; response_revision:20582; }","duration":"167.012537ms","start":"2025-05-22T23:40:54.863802Z","end":"2025-05-22T23:40:55.030814Z","steps":["trace[416367766] 'range keys from in-memory index tree'  (duration: 166.746937ms)"],"step_count":1}
{"level":"info","ts":"2025-05-22T23:40:55.031499Z","caller":"traceutil/trace.go:171","msg":"trace[113419762] transaction","detail":"{read_only:false; response_revision:20583; number_of_response:1; }","duration":"167.542736ms","start":"2025-05-22T23:40:54.863887Z","end":"2025-05-22T23:40:55.031429Z","steps":["trace[113419762] 'process raft request'  (duration: 167.392636ms)"],"step_count":1}
{"level":"info","ts":"2025-05-22T23:40:55.046697Z","caller":"traceutil/trace.go:171","msg":"trace[1815035885] transaction","detail":"{read_only:false; response_revision:20584; number_of_response:1; }","duration":"115.660956ms","start":"2025-05-22T23:40:54.930998Z","end":"2025-05-22T23:40:55.046659Z","steps":["trace[1815035885] 'process raft request'  (duration: 115.423556ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-22T23:40:58.730554Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"169.043136ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kube-system/etcd-minikube\" limit:1 ","response":"range_response_count:1 size:5852"}
{"level":"info","ts":"2025-05-22T23:40:58.730708Z","caller":"traceutil/trace.go:171","msg":"trace[2078126131] range","detail":"{range_begin:/registry/pods/kube-system/etcd-minikube; range_end:; response_count:1; response_revision:20606; }","duration":"169.239636ms","start":"2025-05-22T23:40:58.561440Z","end":"2025-05-22T23:40:58.730680Z","steps":["trace[2078126131] 'range keys from in-memory index tree'  (duration: 168.737236ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-22T23:42:49.663039Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"131.192958ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/default/kubernetes\" limit:1 ","response":"range_response_count:1 size:475"}
{"level":"info","ts":"2025-05-22T23:42:49.663130Z","caller":"traceutil/trace.go:171","msg":"trace[886904356] range","detail":"{range_begin:/registry/endpointslices/default/kubernetes; range_end:; response_count:1; response_revision:20839; }","duration":"131.3432ms","start":"2025-05-22T23:42:49.531770Z","end":"2025-05-22T23:42:49.663113Z","steps":["trace[886904356] 'range keys from in-memory index tree'  (duration: 130.9571ms)"],"step_count":1}
{"level":"info","ts":"2025-05-22T23:44:21.814524Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-05-22T23:44:21.814677Z","caller":"embed/etcd.go:378","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2025-05-22T23:44:21.814867Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-05-22T23:44:21.815148Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-05-22T23:44:22.010675Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"188.714155ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" limit:1 ","response":"","error":"rangeKeys: context cancelled: context canceled"}
{"level":"info","ts":"2025-05-22T23:44:22.010787Z","caller":"traceutil/trace.go:171","msg":"trace[1391466850] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; }","duration":"188.896938ms","start":"2025-05-22T23:44:21.821876Z","end":"2025-05-22T23:44:22.010773Z","steps":["trace[1391466850] 'range keys from in-memory index tree'  (duration: 188.537513ms)"],"step_count":1}
2025/05/22 23:44:22 WARNING: [core] [Server #8] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"warn","ts":"2025-05-22T23:44:22.109803Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-05-22T23:44:22.109854Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-05-22T23:44:22.109956Z","caller":"etcdserver/server.go:1543","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-05-22T23:44:22.117958Z","caller":"embed/etcd.go:582","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-22T23:44:22.118135Z","caller":"embed/etcd.go:587","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-22T23:44:22.118154Z","caller":"embed/etcd.go:380","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [8e6a41186c5c] <==
{"level":"info","ts":"2025-05-23T00:14:07.288114Z","caller":"traceutil/trace.go:171","msg":"trace[2005848903] range","detail":"{range_begin:/registry/deployments/default/user-service-deployment; range_end:; response_count:1; response_revision:21452; }","duration":"430.803892ms","start":"2025-05-23T00:14:06.857287Z","end":"2025-05-23T00:14:07.288091Z","steps":["trace[2005848903] 'range keys from in-memory index tree'  (duration: 430.340242ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T00:14:07.288127Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"123.637101ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2025-05-23T00:14:07.288161Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-23T00:14:06.857175Z","time spent":"430.971366ms","remote":"127.0.0.1:40478","response type":"/etcdserverpb.KV/Range","request count":0,"request size":57,"response count":1,"response size":4060,"request content":"key:\"/registry/deployments/default/user-service-deployment\" limit:1 "}
{"level":"info","ts":"2025-05-23T00:14:07.288175Z","caller":"traceutil/trace.go:171","msg":"trace[1329955971] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:21452; }","duration":"123.769926ms","start":"2025-05-23T00:14:07.164390Z","end":"2025-05-23T00:14:07.288160Z","steps":["trace[1329955971] 'range keys from in-memory index tree'  (duration: 123.624176ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T00:14:49.182890Z","caller":"traceutil/trace.go:171","msg":"trace[1314678615] transaction","detail":"{read_only:false; response_revision:21498; number_of_response:1; }","duration":"149.117131ms","start":"2025-05-23T00:14:49.033739Z","end":"2025-05-23T00:14:49.182857Z","steps":["trace[1314678615] 'process raft request'  (duration: 148.897831ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T00:14:49.352135Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"167.332922ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-05-23T00:14:49.352236Z","caller":"traceutil/trace.go:171","msg":"trace[101012886] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:21498; }","duration":"167.487522ms","start":"2025-05-23T00:14:49.184724Z","end":"2025-05-23T00:14:49.352211Z","steps":["trace[101012886] 'range keys from in-memory index tree'  (duration: 167.204922ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T00:15:36.980129Z","caller":"traceutil/trace.go:171","msg":"trace[442231367] transaction","detail":"{read_only:false; response_revision:21536; number_of_response:1; }","duration":"109.546155ms","start":"2025-05-23T00:15:36.870532Z","end":"2025-05-23T00:15:36.980078Z","steps":["trace[442231367] 'process raft request'  (duration: 104.124439ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T00:16:02.968815Z","caller":"etcdserver/v3_server.go:920","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128037430005631717,"retry-timeout":"500ms"}
{"level":"warn","ts":"2025-05-23T00:16:03.470001Z","caller":"etcdserver/v3_server.go:920","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128037430005631717,"retry-timeout":"500ms"}
{"level":"warn","ts":"2025-05-23T00:16:03.557360Z","caller":"wal/wal.go:805","msg":"slow fdatasync","took":"1.395541633s","expected-duration":"1s"}
{"level":"info","ts":"2025-05-23T00:16:03.557924Z","caller":"traceutil/trace.go:171","msg":"trace[457539658] transaction","detail":"{read_only:false; response_revision:21556; number_of_response:1; }","duration":"1.39626525s","start":"2025-05-23T00:16:02.161628Z","end":"2025-05-23T00:16:03.557893Z","steps":["trace[457539658] 'process raft request'  (duration: 1.396065233s)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T00:16:03.558167Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-23T00:16:02.161596Z","time spent":"1.396483966s","remote":"127.0.0.1:40312","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":520,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:21545 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:471 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2025-05-23T00:16:04.108640Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"309.455199ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128037430005631720 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:21548 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >>","response":"size:18"}
{"level":"info","ts":"2025-05-23T00:16:04.108748Z","caller":"traceutil/trace.go:171","msg":"trace[1084409849] linearizableReadLoop","detail":"{readStateIndex:26363; appliedIndex:26361; }","duration":"1.640350568s","start":"2025-05-23T00:16:02.468375Z","end":"2025-05-23T00:16:04.108726Z","steps":["trace[1084409849] 'read index received'  (duration: 1.089383592s)","trace[1084409849] 'applied index is now lower than readState.Index'  (duration: 550.965051ms)"],"step_count":2}
{"level":"info","ts":"2025-05-23T00:16:04.108855Z","caller":"traceutil/trace.go:171","msg":"trace[930914870] transaction","detail":"{read_only:false; response_revision:21557; number_of_response:1; }","duration":"1.632019626s","start":"2025-05-23T00:16:02.476823Z","end":"2025-05-23T00:16:04.108843Z","steps":["trace[930914870] 'process raft request'  (duration: 1.322185569s)","trace[930914870] 'compare'  (duration: 309.234832ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-23T00:16:04.108940Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-23T00:16:02.476792Z","time spent":"1.632095892s","remote":"127.0.0.1:40312","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:21548 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"warn","ts":"2025-05-23T00:16:04.109138Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.640754084s","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-05-23T00:16:04.109179Z","caller":"traceutil/trace.go:171","msg":"trace[1517130673] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:21557; }","duration":"1.640801751s","start":"2025-05-23T00:16:02.468365Z","end":"2025-05-23T00:16:04.109167Z","steps":["trace[1517130673] 'agreement among raft nodes before linearized reading'  (duration: 1.640736942s)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T00:16:04.109225Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.439789315s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-05-23T00:16:04.109293Z","caller":"traceutil/trace.go:171","msg":"trace[2084382943] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:21557; }","duration":"1.43988199s","start":"2025-05-23T00:16:02.669389Z","end":"2025-05-23T00:16:04.109270Z","steps":["trace[2084382943] 'agreement among raft nodes before linearized reading'  (duration: 1.439773823s)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T00:16:04.109340Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-23T00:16:02.669368Z","time spent":"1.439959357s","remote":"127.0.0.1:40046","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-05-23T00:16:04.791137Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"382.686963ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128037430005631727 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:21555 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:18"}
{"level":"info","ts":"2025-05-23T00:16:04.791282Z","caller":"traceutil/trace.go:171","msg":"trace[522162622] transaction","detail":"{read_only:false; response_revision:21558; number_of_response:1; }","duration":"635.878447ms","start":"2025-05-23T00:16:04.155379Z","end":"2025-05-23T00:16:04.791257Z","steps":["trace[522162622] 'process raft request'  (duration: 252.922534ms)","trace[522162622] 'compare'  (duration: 381.929338ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-23T00:16:04.791379Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-23T00:16:04.155354Z","time spent":"635.972772ms","remote":"127.0.0.1:40216","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1094,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:21555 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2025-05-23T00:16:04.792352Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"322.159923ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-05-23T00:16:04.792465Z","caller":"traceutil/trace.go:171","msg":"trace[1370862352] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:21557; }","duration":"324.249924ms","start":"2025-05-23T00:16:04.468196Z","end":"2025-05-23T00:16:04.792446Z","steps":["trace[1370862352] 'range keys from in-memory index tree'  (duration: 322.141774ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T00:16:07.653469Z","caller":"traceutil/trace.go:171","msg":"trace[370174261] linearizableReadLoop","detail":"{readStateIndex:26366; appliedIndex:26365; }","duration":"185.352561ms","start":"2025-05-23T00:16:07.468088Z","end":"2025-05-23T00:16:07.653441Z","steps":["trace[370174261] 'read index received'  (duration: 140.477329ms)","trace[370174261] 'applied index is now lower than readState.Index'  (duration: 44.873674ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-23T00:16:07.653670Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"185.562202ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-05-23T00:16:07.653733Z","caller":"traceutil/trace.go:171","msg":"trace[1225441159] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:21559; }","duration":"185.635353ms","start":"2025-05-23T00:16:07.468078Z","end":"2025-05-23T00:16:07.653714Z","steps":["trace[1225441159] 'agreement among raft nodes before linearized reading'  (duration: 185.504086ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T00:16:17.579058Z","caller":"traceutil/trace.go:171","msg":"trace[1444608353] linearizableReadLoop","detail":"{readStateIndex:26377; appliedIndex:26376; }","duration":"110.020988ms","start":"2025-05-23T00:16:17.469009Z","end":"2025-05-23T00:16:17.579030Z","steps":["trace[1444608353] 'read index received'  (duration: 27.877116ms)","trace[1444608353] 'applied index is now lower than readState.Index'  (duration: 82.142406ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-23T00:16:17.579277Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"110.220731ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-05-23T00:16:17.579326Z","caller":"traceutil/trace.go:171","msg":"trace[517858299] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:21568; }","duration":"110.326972ms","start":"2025-05-23T00:16:17.468982Z","end":"2025-05-23T00:16:17.579309Z","steps":["trace[517858299] 'agreement among raft nodes before linearized reading'  (duration: 110.180855ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T00:16:22.123460Z","caller":"traceutil/trace.go:171","msg":"trace[1104080124] transaction","detail":"{read_only:false; response_revision:21569; number_of_response:1; }","duration":"264.031525ms","start":"2025-05-23T00:16:19.133524Z","end":"2025-05-23T00:16:22.123437Z","steps":["trace[1104080124] 'process raft request'  (duration: 263.857908ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T00:16:27.927369Z","caller":"traceutil/trace.go:171","msg":"trace[729073635] transaction","detail":"{read_only:false; response_revision:21574; number_of_response:1; }","duration":"443.940095ms","start":"2025-05-23T00:16:27.483379Z","end":"2025-05-23T00:16:27.927319Z","steps":["trace[729073635] 'process raft request'  (duration: 443.74842ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T00:16:27.927563Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-23T00:16:27.483345Z","time spent":"444.11417ms","remote":"127.0.0.1:40312","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:21566 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"info","ts":"2025-05-23T00:16:27.927679Z","caller":"traceutil/trace.go:171","msg":"trace[526987150] linearizableReadLoop","detail":"{readStateIndex:26384; appliedIndex:26384; }","duration":"352.704631ms","start":"2025-05-23T00:16:27.574949Z","end":"2025-05-23T00:16:27.927653Z","steps":["trace[526987150] 'read index received'  (duration: 352.69519ms)","trace[526987150] 'applied index is now lower than readState.Index'  (duration: 7.883¬µs)"],"step_count":2}
{"level":"warn","ts":"2025-05-23T00:16:27.927840Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"352.866973ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/prioritylevelconfigurations/\" range_end:\"/registry/prioritylevelconfigurations0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2025-05-23T00:16:27.927895Z","caller":"traceutil/trace.go:171","msg":"trace[979863140] range","detail":"{range_begin:/registry/prioritylevelconfigurations/; range_end:/registry/prioritylevelconfigurations0; response_count:0; response_revision:21574; }","duration":"352.948189ms","start":"2025-05-23T00:16:27.574932Z","end":"2025-05-23T00:16:27.927880Z","steps":["trace[979863140] 'agreement among raft nodes before linearized reading'  (duration: 352.801523ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T00:16:27.927942Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-23T00:16:27.574856Z","time spent":"353.070381ms","remote":"127.0.0.1:40470","response type":"/etcdserverpb.KV/Range","request count":0,"request size":82,"response count":8,"response size":32,"request content":"key:\"/registry/prioritylevelconfigurations/\" range_end:\"/registry/prioritylevelconfigurations0\" count_only:true "}
{"level":"warn","ts":"2025-05-23T00:16:28.573236Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"172.08757ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-05-23T00:16:28.573340Z","caller":"traceutil/trace.go:171","msg":"trace[1334696968] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:21574; }","duration":"172.202428ms","start":"2025-05-23T00:16:28.401115Z","end":"2025-05-23T00:16:28.573317Z","steps":["trace[1334696968] 'range keys from in-memory index tree'  (duration: 171.951812ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T00:16:28.573601Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"371.558631ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-05-23T00:16:28.573666Z","caller":"traceutil/trace.go:171","msg":"trace[1208378240] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:21574; }","duration":"371.632697ms","start":"2025-05-23T00:16:28.202015Z","end":"2025-05-23T00:16:28.573648Z","steps":["trace[1208378240] 'range keys from in-memory index tree'  (duration: 371.506197ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T00:16:28.575590Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"335.580932ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1112"}
{"level":"info","ts":"2025-05-23T00:16:28.575691Z","caller":"traceutil/trace.go:171","msg":"trace[594747552] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:21574; }","duration":"335.73044ms","start":"2025-05-23T00:16:28.239939Z","end":"2025-05-23T00:16:28.575669Z","steps":["trace[594747552] 'range keys from in-memory index tree'  (duration: 333.880332ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T00:16:28.575885Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-23T00:16:28.239915Z","time spent":"335.814957ms","remote":"127.0.0.1:40216","response type":"/etcdserverpb.KV/Range","request count":0,"request size":69,"response count":1,"response size":1136,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 "}
{"level":"warn","ts":"2025-05-23T00:16:29.982479Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"293.911374ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/flowschemas/\" range_end:\"/registry/flowschemas0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"warn","ts":"2025-05-23T00:16:29.982928Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"779.56686ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-05-23T00:16:29.983003Z","caller":"traceutil/trace.go:171","msg":"trace[140274536] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:21575; }","duration":"779.648718ms","start":"2025-05-23T00:16:29.203333Z","end":"2025-05-23T00:16:29.982982Z","steps":["trace[140274536] 'range keys from in-memory index tree'  (duration: 779.472535ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T00:16:29.983201Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"927.464514ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/\" range_end:\"/registry/clusterroles0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"warn","ts":"2025-05-23T00:16:29.983640Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"582.070208ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-05-23T00:16:29.983886Z","caller":"traceutil/trace.go:171","msg":"trace[619772052] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:21575; }","duration":"582.353824ms","start":"2025-05-23T00:16:29.401511Z","end":"2025-05-23T00:16:29.983865Z","steps":["trace[619772052] 'range keys from in-memory index tree'  (duration: 581.972583ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T00:16:29.983947Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-23T00:16:29.401486Z","time spent":"582.442466ms","remote":"127.0.0.1:40046","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-05-23T00:16:30.060160Z","caller":"traceutil/trace.go:171","msg":"trace[65399737] range","detail":"{range_begin:/registry/clusterroles/; range_end:/registry/clusterroles0; response_count:0; response_revision:21575; }","duration":"927.551689ms","start":"2025-05-23T00:16:29.055709Z","end":"2025-05-23T00:16:29.983261Z","steps":["trace[65399737] 'count revisions from in-memory index tree'  (duration: 927.372205ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T00:16:30.060308Z","caller":"traceutil/trace.go:171","msg":"trace[2072825456] range","detail":"{range_begin:/registry/flowschemas/; range_end:/registry/flowschemas0; response_count:0; response_revision:21575; }","duration":"294.130091ms","start":"2025-05-23T00:16:29.688454Z","end":"2025-05-23T00:16:29.982584Z","steps":["trace[2072825456] 'count revisions from in-memory index tree'  (duration: 293.769474ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T00:16:30.060413Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-23T00:16:29.055642Z","time spent":"1.004717419s","remote":"127.0.0.1:40382","response type":"/etcdserverpb.KV/Range","request count":0,"request size":52,"response count":69,"response size":32,"request content":"key:\"/registry/clusterroles/\" range_end:\"/registry/clusterroles0\" count_only:true "}
{"level":"warn","ts":"2025-05-23T00:16:30.060480Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-23T00:16:29.688429Z","time spent":"372.018706ms","remote":"127.0.0.1:40460","response type":"/etcdserverpb.KV/Range","request count":0,"request size":50,"response count":13,"response size":32,"request content":"key:\"/registry/flowschemas/\" range_end:\"/registry/flowschemas0\" count_only:true "}
{"level":"warn","ts":"2025-05-23T00:16:30.360437Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"103.463522ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128037430005631836 > lease_revoke:<id:70cc96fa78908b0d>","response":"size:30"}
{"level":"info","ts":"2025-05-23T00:16:30.767037Z","caller":"traceutil/trace.go:171","msg":"trace[1602864667] transaction","detail":"{read_only:false; response_revision:21576; number_of_response:1; }","duration":"144.186804ms","start":"2025-05-23T00:16:30.622767Z","end":"2025-05-23T00:16:30.766953Z","steps":["trace[1602864667] 'process raft request'  (duration: 142.287013ms)"],"step_count":1}


==> kernel <==
 00:17:19 up  8:18,  0 users,  load average: 4.00, 4.71, 4.08
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [3d901800b3f9] <==
I0522 23:44:22.019809       1 controller.go:157] Shutting down quota evaluator
I0522 23:44:22.019822       1 controller.go:176] quota evaluator worker shutdown
I0522 23:44:22.019895       1 controller.go:176] quota evaluator worker shutdown
I0522 23:44:22.019908       1 controller.go:176] quota evaluator worker shutdown
I0522 23:44:22.019917       1 controller.go:176] quota evaluator worker shutdown
I0522 23:44:22.019923       1 controller.go:176] quota evaluator worker shutdown
W0522 23:44:22.829319       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.829386       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.829446       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.829510       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.829741       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.829835       1 logging.go:55] [core] [Channel #7 SubChannel #8]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.830966       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.831138       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.918525       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.918614       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.922166       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.922163       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.923925       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.923938       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.923940       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.924390       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.926581       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.926640       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.926609       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.928327       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.928381       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.932310       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.932455       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.932470       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.932562       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.933327       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.933915       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.934214       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.934246       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.934564       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.937126       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.937221       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.937164       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.937290       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.937389       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.937408       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.937394       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.937501       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.937663       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.937714       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.937714       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.937735       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.937693       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.937669       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.938174       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.938482       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.938511       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.938516       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.938722       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.938759       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.940135       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.940154       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:22.940407       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0522 23:44:23.010924       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [ee7598378023] <==
I0523 00:08:51.482509       1 aggregator.go:171] initial CRD sync complete...
I0523 00:08:51.482520       1 autoregister_controller.go:144] Starting autoregister controller
I0523 00:08:51.482526       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0523 00:08:51.490649       1 controller.go:142] Starting OpenAPI controller
I0523 00:08:51.490799       1 controller.go:90] Starting OpenAPI V3 controller
I0523 00:08:51.490842       1 naming_controller.go:294] Starting NamingConditionController
I0523 00:08:51.490869       1 establishing_controller.go:81] Starting EstablishingController
I0523 00:08:51.491193       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0523 00:08:51.583924       1 cache.go:39] Caches are synced for LocalAvailability controller
I0523 00:08:51.584202       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0523 00:08:51.586253       1 cache.go:39] Caches are synced for autoregister controller
I0523 00:08:51.597173       1 shared_informer.go:320] Caches are synced for configmaps
I0523 00:08:51.598450       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
W0523 00:08:51.598505       1 handler_proxy.go:99] no RequestInfo found in the context
I0523 00:08:51.598557       1 handler.go:286] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
E0523 00:08:51.598581       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0523 00:08:51.599194       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0523 00:08:51.599214       1 policy_source.go:240] refreshing policies
I0523 00:08:51.599453       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0523 00:08:51.621537       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0523 00:08:51.681310       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0523 00:08:51.681341       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0523 00:08:51.684355       1 shared_informer.go:320] Caches are synced for node_authorizer
I0523 00:08:52.192887       1 controller.go:615] quota admission added evaluator for: serviceaccounts
W0523 00:08:52.507077       1 handler_proxy.go:99] no RequestInfo found in the context
E0523 00:08:52.507213       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
W0523 00:08:52.507082       1 handler_proxy.go:99] no RequestInfo found in the context
E0523 00:08:52.507442       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0523 00:08:52.510464       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0523 00:08:52.584092       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0523 00:08:52.600999       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0523 00:08:55.111963       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0523 00:09:02.215302       1 controller.go:615] quota admission added evaluator for: endpoints
I0523 00:09:02.226147       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0523 00:09:02.226283       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0523 00:09:02.402278       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
W0523 00:09:56.130234       1 handler_proxy.go:99] no RequestInfo found in the context
E0523 00:09:56.130385       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
I0523 00:09:56.131734       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0523 00:09:56.203583       1 handler_proxy.go:99] no RequestInfo found in the context
E0523 00:09:56.203891       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0523 00:09:56.205327       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0523 00:11:19.354043       1 handler_proxy.go:99] no RequestInfo found in the context
E0523 00:11:19.354048       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.101.144.112:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.101.144.112:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.101.144.112:443: connect: connection refused" logger="UnhandledError"
E0523 00:11:19.354222       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
E0523 00:11:19.429155       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: Operation cannot be fulfilled on apiservices.apiregistration.k8s.io \"v1beta1.metrics.k8s.io\": the object has been modified; please apply your changes to the latest version and try again" logger="UnhandledError"
I0523 00:11:19.435103       1 handler.go:286] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0523 00:11:26.382336       1 controller.go:615] quota admission added evaluator for: horizontalpodautoscalers.autoscaling


==> kube-controller-manager [24684a8a1aaa] <==
E0523 00:13:34.072294       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/help-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container help-service of Pod help-service-deployment-986dffd9d-8hbzs" logger="UnhandledError"
E0523 00:13:34.082489       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/user-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container user-service of Pod user-service-deployment-799db9c9f6-4kvt5" logger="UnhandledError"
W0523 00:13:51.814492       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
W0523 00:13:51.815077       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0523 00:13:51.852269       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/help-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container help-service of Pod help-service-deployment-986dffd9d-8hbzs" logger="UnhandledError"
E0523 00:13:51.852265       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/user-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container user-service of Pod user-service-deployment-799db9c9f6-4kvt5" logger="UnhandledError"
W0523 00:14:07.296654       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
W0523 00:14:07.297139       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0523 00:14:07.358085       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/user-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container user-service of Pod user-service-deployment-799db9c9f6-4kvt5" logger="UnhandledError"
E0523 00:14:07.358405       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/help-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container help-service of Pod help-service-deployment-986dffd9d-8hbzs" logger="UnhandledError"
I0523 00:14:08.685235       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/frontend-deployment-6588ccddd" duration="39.826948ms"
I0523 00:14:08.685769       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/frontend-deployment-6588ccddd" duration="109.541¬µs"
W0523 00:13:23.996699       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
W0523 00:13:24.009880       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0523 00:13:24.104036       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/user-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container user-service of Pod user-service-deployment-799db9c9f6-4kvt5" logger="UnhandledError"
E0523 00:13:24.104712       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/help-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container help-service of Pod help-service-deployment-986dffd9d-8hbzs" logger="UnhandledError"
W0523 00:14:40.218732       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
W0523 00:14:40.219415       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0523 00:14:40.223729       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/help-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container help-service of Pod help-service-deployment-986dffd9d-8hbzs" logger="UnhandledError"
E0523 00:14:40.229416       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/user-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container user-service of Pod user-service-deployment-799db9c9f6-cj7kj" logger="UnhandledError"
I0523 00:14:44.095214       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
W0523 00:14:55.281273       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
W0523 00:14:55.281273       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0523 00:14:55.295484       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/user-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container user-service of Pod user-service-deployment-799db9c9f6-cj7kj" logger="UnhandledError"
E0523 00:14:55.297653       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/help-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container help-service of Pod help-service-deployment-986dffd9d-8hbzs" logger="UnhandledError"
W0523 00:15:10.309700       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
W0523 00:15:10.313793       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0523 00:15:10.327894       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/help-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container help-service of Pod help-service-deployment-986dffd9d-8hbzs" logger="UnhandledError"
E0523 00:15:10.336529       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/user-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container user-service of Pod user-service-deployment-799db9c9f6-4kvt5" logger="UnhandledError"
W0523 00:15:26.206205       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
W0523 00:15:26.206747       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0523 00:15:26.219601       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/help-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container help-service of Pod help-service-deployment-986dffd9d-8hbzs" logger="UnhandledError"
E0523 00:15:26.227701       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/user-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container user-service of Pod user-service-deployment-799db9c9f6-cj7kj" logger="UnhandledError"
W0523 00:15:41.228929       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
W0523 00:15:41.232199       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0523 00:15:41.238642       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/user-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container user-service of Pod user-service-deployment-799db9c9f6-cj7kj" logger="UnhandledError"
E0523 00:15:41.238756       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/help-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container help-service of Pod help-service-deployment-986dffd9d-8hbzs" logger="UnhandledError"
W0523 00:15:58.982715       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
W0523 00:15:58.982742       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0523 00:15:58.992405       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/help-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container help-service of Pod help-service-deployment-986dffd9d-8hbzs" logger="UnhandledError"
E0523 00:15:59.047263       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/user-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container user-service of Pod user-service-deployment-799db9c9f6-4kvt5" logger="UnhandledError"
W0523 00:16:14.001027       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0523 00:16:14.014577       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/help-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container help-service of Pod help-service-deployment-986dffd9d-8hbzs" logger="UnhandledError"
W0523 00:16:14.018531       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0523 00:16:14.033398       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/user-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container user-service of Pod user-service-deployment-799db9c9f6-cj7kj" logger="UnhandledError"
W0523 00:16:32.434106       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
W0523 00:16:32.689821       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0523 00:16:32.707029       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/user-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container user-service of Pod user-service-deployment-799db9c9f6-cj7kj" logger="UnhandledError"
E0523 00:16:32.707670       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/help-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container help-service of Pod help-service-deployment-986dffd9d-8hbzs" logger="UnhandledError"
I0523 00:16:39.551414       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/frontend-deployment-6588ccddd" duration="109.839947ms"
I0523 00:16:39.551847       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/frontend-deployment-6588ccddd" duration="323.767¬µs"
I0523 00:16:42.240662       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
W0523 00:16:47.716817       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
W0523 00:16:47.717319       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0523 00:16:47.724649       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/help-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container help-service of Pod help-service-deployment-986dffd9d-8hbzs" logger="UnhandledError"
E0523 00:16:47.754607       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/user-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container user-service of Pod user-service-deployment-799db9c9f6-4kvt5" logger="UnhandledError"
W0523 00:16:04.363894       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0523 00:16:04.374376       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/help-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container help-service of Pod help-service-deployment-986dffd9d-8hbzs" logger="UnhandledError"
W0523 00:16:04.383680       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0523 00:16:04.389231       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/user-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container user-service of Pod user-service-deployment-799db9c9f6-4kvt5" logger="UnhandledError"


==> kube-controller-manager [3c6724f18973] <==
I0522 23:41:53.818484       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="7.907535ms"
I0522 23:41:53.818583       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="39.05¬µs"
W0522 23:42:00.435729       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
W0522 23:42:00.435729       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0522 23:42:00.436895       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/user-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)" logger="UnhandledError"
E0522 23:42:00.437000       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/help-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)" logger="UnhandledError"
I0522 23:42:00.469115       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0522 23:42:00.547994       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0522 23:42:07.954432       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/frontend-deployment-6588ccddd" duration="134.292¬µs"
I0522 23:42:08.685323       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/help-service-deployment-986dffd9d" duration="54.083¬µs"
W0522 23:42:15.438465       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
W0522 23:42:15.438549       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0522 23:42:15.440617       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/help-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)" logger="UnhandledError"
E0522 23:42:15.440617       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/user-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)" logger="UnhandledError"
I0522 23:42:20.960416       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/help-service-deployment-986dffd9d" duration="68.567¬µs"
I0522 23:42:23.973597       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/user-service-deployment-799db9c9f6" duration="60.867¬µs"
W0522 23:42:33.184218       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
W0522 23:42:33.185464       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0522 23:42:33.186081       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/user-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)" logger="UnhandledError"
E0522 23:42:33.191101       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/help-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)" logger="UnhandledError"
I0522 23:42:33.217600       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0522 23:42:33.285421       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0522 23:42:37.682981       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/user-service-deployment-799db9c9f6" duration="61.417¬µs"
I0522 23:42:42.909867       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/help-service-deployment-986dffd9d" duration="45.65¬µs"
W0522 23:42:48.193478       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0522 23:42:48.194767       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/user-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)" logger="UnhandledError"
W0522 23:42:48.196714       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0522 23:42:48.198072       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/help-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)" logger="UnhandledError"
I0522 23:42:54.685758       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/help-service-deployment-986dffd9d" duration="71.684¬µs"
I0522 23:43:01.905995       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/frontend-deployment-6588ccddd" duration="81.767¬µs"
W0522 23:43:05.931430       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
W0522 23:43:05.931509       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0522 23:43:05.932413       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/help-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)" logger="UnhandledError"
E0522 23:43:05.932559       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/user-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)" logger="UnhandledError"
I0522 23:43:05.953029       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0522 23:43:06.017412       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0522 23:43:12.408887       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/frontend-deployment-6588ccddd" duration="72.233¬µs"
I0522 23:43:18.209264       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/user-service-deployment-799db9c9f6" duration="55.642¬µs"
W0522 23:43:20.941045       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
W0522 23:43:20.941666       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0522 23:43:20.942657       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/user-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)" logger="UnhandledError"
E0522 23:43:20.943411       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/help-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)" logger="UnhandledError"
I0522 23:43:29.418554       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/user-service-deployment-799db9c9f6" duration="47.941¬µs"
W0522 23:43:38.687167       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
W0522 23:43:38.687185       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0522 23:43:38.690942       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/help-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)" logger="UnhandledError"
E0522 23:43:38.691445       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/user-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)" logger="UnhandledError"
I0522 23:43:38.695882       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0522 23:43:38.752403       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
W0522 23:43:53.700812       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
W0522 23:43:53.700838       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0522 23:43:53.702826       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/user-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)" logger="UnhandledError"
E0522 23:43:53.702850       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/help-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)" logger="UnhandledError"
I0522 23:44:00.573564       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7fbb699795" duration="60.225¬µs"
I0522 23:44:11.410686       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
W0522 23:44:11.416907       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
W0522 23:44:11.416915       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E0522 23:44:11.417876       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/user-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)" logger="UnhandledError"
E0522 23:44:11.418110       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/help-service-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)" logger="UnhandledError"
E0522 23:44:11.462099       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"


==> kube-proxy [35187730bef2] <==
I0523 00:09:07.409520       1 server_linux.go:66] "Using iptables proxy"
I0523 00:09:07.761559       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0523 00:09:07.761769       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0523 00:09:07.829487       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0523 00:09:07.829601       1 server_linux.go:170] "Using iptables Proxier"
I0523 00:09:07.835611       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0523 00:09:07.848468       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0523 00:09:07.861283       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0523 00:09:07.862580       1 server.go:497] "Version info" version="v1.32.0"
I0523 00:09:07.862701       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0523 00:09:07.876526       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0523 00:09:07.888648       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0523 00:09:07.894521       1 config.go:105] "Starting endpoint slice config controller"
I0523 00:09:07.894744       1 config.go:199] "Starting service config controller"
I0523 00:09:07.896555       1 config.go:329] "Starting node config controller"
I0523 00:09:07.898940       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0523 00:09:07.898937       1 shared_informer.go:313] Waiting for caches to sync for node config
I0523 00:09:07.899167       1 shared_informer.go:313] Waiting for caches to sync for service config
I0523 00:09:07.999948       1 shared_informer.go:320] Caches are synced for service config
I0523 00:09:07.999995       1 shared_informer.go:320] Caches are synced for node config
I0523 00:09:08.000053       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-proxy [63984bbaf47f] <==
I0522 23:41:00.392348       1 server_linux.go:66] "Using iptables proxy"
I0522 23:41:00.870319       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0522 23:41:00.870747       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0522 23:41:00.963478       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0522 23:41:00.963584       1 server_linux.go:170] "Using iptables Proxier"
I0522 23:41:00.987183       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0522 23:41:01.001824       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0522 23:41:01.026691       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0522 23:41:01.030452       1 server.go:497] "Version info" version="v1.32.0"
I0522 23:41:01.030572       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0522 23:41:01.050366       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0522 23:41:01.070158       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0522 23:41:01.080617       1 config.go:199] "Starting service config controller"
I0522 23:41:01.080970       1 config.go:105] "Starting endpoint slice config controller"
I0522 23:41:01.081877       1 config.go:329] "Starting node config controller"
I0522 23:41:01.082544       1 shared_informer.go:313] Waiting for caches to sync for service config
I0522 23:41:01.082678       1 shared_informer.go:313] Waiting for caches to sync for node config
I0522 23:41:01.083068       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0522 23:41:01.183140       1 shared_informer.go:320] Caches are synced for node config
I0522 23:41:01.183242       1 shared_informer.go:320] Caches are synced for service config
I0522 23:41:01.198811       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [10e77022a389] <==
I0522 23:40:44.072392       1 serving.go:386] Generated self-signed cert in-memory
I0522 23:40:48.169340       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0522 23:40:48.169442       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0522 23:40:48.284444       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0522 23:40:48.284802       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0522 23:40:48.285201       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0522 23:40:48.285181       1 requestheader_controller.go:180] Starting RequestHeaderAuthRequestController
I0522 23:40:48.288063       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0522 23:40:48.374009       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0522 23:40:48.382788       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0522 23:40:48.374888       1 shared_informer.go:313] Waiting for caches to sync for RequestHeaderAuthRequestController
I0522 23:40:48.560778       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0522 23:40:48.561377       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0522 23:40:48.565364       1 shared_informer.go:320] Caches are synced for RequestHeaderAuthRequestController
I0522 23:44:21.811477       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I0522 23:44:21.811950       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0522 23:44:21.812631       1 requestheader_controller.go:194] Shutting down RequestHeaderAuthRequestController
I0522 23:44:21.812723       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0522 23:44:21.812801       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0522 23:44:21.813756       1 run.go:72] "command failed" err="finished without leader elect"


==> kube-scheduler [21066fc74a3f] <==
I0523 00:08:48.496206       1 serving.go:386] Generated self-signed cert in-memory
W0523 00:08:51.586629       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0523 00:08:51.586688       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0523 00:08:51.586703       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0523 00:08:51.586727       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0523 00:08:51.711390       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0523 00:08:51.711424       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0523 00:08:51.715219       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0523 00:08:51.716733       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0523 00:08:51.781372       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0523 00:08:51.781558       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0523 00:08:51.881967       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
May 23 00:09:23 minikube kubelet[1708]: E0523 00:09:23.373502    1708 helpers.go:851] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
May 23 00:09:36 minikube kubelet[1708]: E0523 00:09:36.204835    1708 summary_sys_containers.go:51] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
May 23 00:09:36 minikube kubelet[1708]: E0523 00:09:36.204961    1708 helpers.go:851] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
May 23 00:09:39 minikube kubelet[1708]: I0523 00:09:39.889734    1708 scope.go:117] "RemoveContainer" containerID="d384e76e9f9fcb70ce9607c7b0f3b38f32499bdc80a0b1c09500f0afc0aba02b"
May 23 00:09:39 minikube kubelet[1708]: I0523 00:09:39.890431    1708 scope.go:117] "RemoveContainer" containerID="f2deeac894ef2c6f38c0d7756b099039f3bd2b766a583bf7f0d0822ffeee2065"
May 23 00:09:39 minikube kubelet[1708]: E0523 00:09:39.890835    1708 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with CrashLoopBackOff: \"back-off 10s restarting failed container=kubernetes-dashboard pod=kubernetes-dashboard-7779f9b69b-9lfsr_kubernetes-dashboard(75d5dc48-f845-4d74-9f2f-8a3db04b0463)\"" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-9lfsr" podUID="75d5dc48-f845-4d74-9f2f-8a3db04b0463"
May 23 00:09:40 minikube kubelet[1708]: I0523 00:09:40.018197    1708 scope.go:117] "RemoveContainer" containerID="748a57e2d21a2377f54739106bbefcc1b8778301ac11a17bc346fe18c0e1053b"
May 23 00:09:40 minikube kubelet[1708]: E0523 00:09:40.018879    1708 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 20s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(f0a13351-6b27-484e-9869-2d4b935f4416)\"" pod="kube-system/storage-provisioner" podUID="f0a13351-6b27-484e-9869-2d4b935f4416"
May 23 00:09:40 minikube kubelet[1708]: I0523 00:09:40.243985    1708 scope.go:117] "RemoveContainer" containerID="01794cab8cb3a3fd8f2e3dfc57d938a5dc48c1e9793fa7a054b3d9a8943fdd3a"
May 23 00:09:42 minikube kubelet[1708]: I0523 00:09:42.298652    1708 scope.go:117] "RemoveContainer" containerID="1493a23b65620a851b2d665d5fdff164295f3cefeaae7c1674f8ec4d78feec0b"
May 23 00:09:42 minikube kubelet[1708]: I0523 00:09:42.299799    1708 scope.go:117] "RemoveContainer" containerID="97b03db8e471718d82685fd589a3fdb2e9245c04e6aa6ca4b1a099101aafb188"
May 23 00:09:42 minikube kubelet[1708]: E0523 00:09:42.300196    1708 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with CrashLoopBackOff: \"back-off 10s restarting failed container=metrics-server pod=metrics-server-7fbb699795-nlw78_kube-system(df2250cd-99e1-4529-83a8-7c853d5f7fd8)\"" pod="kube-system/metrics-server-7fbb699795-nlw78" podUID="df2250cd-99e1-4529-83a8-7c853d5f7fd8"
May 23 00:09:46 minikube kubelet[1708]: I0523 00:09:46.322445    1708 scope.go:117] "RemoveContainer" containerID="f2deeac894ef2c6f38c0d7756b099039f3bd2b766a583bf7f0d0822ffeee2065"
May 23 00:09:46 minikube kubelet[1708]: E0523 00:09:46.322865    1708 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with CrashLoopBackOff: \"back-off 10s restarting failed container=kubernetes-dashboard pod=kubernetes-dashboard-7779f9b69b-9lfsr_kubernetes-dashboard(75d5dc48-f845-4d74-9f2f-8a3db04b0463)\"" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-9lfsr" podUID="75d5dc48-f845-4d74-9f2f-8a3db04b0463"
May 23 00:09:46 minikube kubelet[1708]: I0523 00:09:46.512371    1708 scope.go:117] "RemoveContainer" containerID="97b03db8e471718d82685fd589a3fdb2e9245c04e6aa6ca4b1a099101aafb188"
May 23 00:09:46 minikube kubelet[1708]: E0523 00:09:46.512749    1708 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with CrashLoopBackOff: \"back-off 10s restarting failed container=metrics-server pod=metrics-server-7fbb699795-nlw78_kube-system(df2250cd-99e1-4529-83a8-7c853d5f7fd8)\"" pod="kube-system/metrics-server-7fbb699795-nlw78" podUID="df2250cd-99e1-4529-83a8-7c853d5f7fd8"
May 23 00:09:50 minikube kubelet[1708]: I0523 00:09:50.816450    1708 scope.go:117] "RemoveContainer" containerID="748a57e2d21a2377f54739106bbefcc1b8778301ac11a17bc346fe18c0e1053b"
May 23 00:09:50 minikube kubelet[1708]: E0523 00:09:50.816890    1708 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 20s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(f0a13351-6b27-484e-9869-2d4b935f4416)\"" pod="kube-system/storage-provisioner" podUID="f0a13351-6b27-484e-9869-2d4b935f4416"
May 23 00:10:00 minikube kubelet[1708]: I0523 00:10:00.552557    1708 scope.go:117] "RemoveContainer" containerID="f2deeac894ef2c6f38c0d7756b099039f3bd2b766a583bf7f0d0822ffeee2065"
May 23 00:10:02 minikube kubelet[1708]: I0523 00:10:02.552489    1708 scope.go:117] "RemoveContainer" containerID="97b03db8e471718d82685fd589a3fdb2e9245c04e6aa6ca4b1a099101aafb188"
May 23 00:10:05 minikube kubelet[1708]: I0523 00:10:05.552667    1708 scope.go:117] "RemoveContainer" containerID="748a57e2d21a2377f54739106bbefcc1b8778301ac11a17bc346fe18c0e1053b"
May 23 00:14:02 minikube kubelet[1708]: E0523 00:14:02.356765    1708 kubelet.go:2579] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.099s"
May 23 00:13:17 minikube kubelet[1708]: E0523 00:13:17.698459    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959197663500270 - 1747959245946385000)"
May 23 00:13:17 minikube kubelet[1708]: E0523 00:13:17.698781    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959197663500270 - 1747959245889314368)"
May 23 00:13:17 minikube kubelet[1708]: E0523 00:13:17.698870    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959197663658570 - 1747959245945919425)"
May 23 00:13:17 minikube kubelet[1708]: E0523 00:13:17.698919    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959197663812870 - 1747959245897234456)"
May 23 00:13:17 minikube kubelet[1708]: E0523 00:13:17.699021    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959197663812770 - 1747959245947155183)"
May 23 00:13:17 minikube kubelet[1708]: E0523 00:13:17.699119    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959197665799469 - 1747959245946319917)"
May 23 00:13:17 minikube kubelet[1708]: E0523 00:13:17.699203    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959197665799469 - 1747959245896982465)"
May 23 00:13:17 minikube kubelet[1708]: E0523 00:13:17.699302    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959197665805469 - 1747959245949552540)"
May 23 00:13:17 minikube kubelet[1708]: E0523 00:13:17.699397    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959197666049569 - 1747959245946724075)"
May 23 00:13:17 minikube kubelet[1708]: E0523 00:13:17.699539    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959197673974465 - 1747959245956028420)"
May 23 00:13:17 minikube kubelet[1708]: E0523 00:13:17.699641    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959197674166365 - 1747959245896608281)"
May 23 00:13:17 minikube kubelet[1708]: E0523 00:13:17.699833    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959197674280265 - 1747959245889564251)"
May 23 00:13:27 minikube kubelet[1708]: E0523 00:13:27.795251    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959207748597585 - 1747959245896982465)"
May 23 00:13:27 minikube kubelet[1708]: E0523 00:13:27.795524    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959207749433585 - 1747959245889564251)"
May 23 00:13:27 minikube kubelet[1708]: E0523 00:13:27.795622    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959207749454785 - 1747959245946385000)"
May 23 00:13:27 minikube kubelet[1708]: E0523 00:13:27.795809    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959207749821785 - 1747959245949552540)"
May 23 00:13:27 minikube kubelet[1708]: E0523 00:13:27.795917    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959207749967885 - 1747959245897234456)"
May 23 00:13:27 minikube kubelet[1708]: E0523 00:13:27.796105    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959207761220879 - 1747959245896608281)"
May 23 00:13:27 minikube kubelet[1708]: E0523 00:13:27.796224    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959207761450879 - 1747959245946319917)"
May 23 00:13:27 minikube kubelet[1708]: E0523 00:13:27.796341    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959207761535179 - 1747959245945919425)"
May 23 00:13:27 minikube kubelet[1708]: E0523 00:13:27.796424    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959207762879079 - 1747959245956028420)"
May 23 00:13:27 minikube kubelet[1708]: E0523 00:13:27.796532    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959207763017779 - 1747959245889314368)"
May 23 00:13:27 minikube kubelet[1708]: E0523 00:13:27.796637    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959207763206079 - 1747959245947155183)"
May 23 00:13:27 minikube kubelet[1708]: E0523 00:13:27.796756    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959207763307678 - 1747959245946724075)"
May 23 00:15:58 minikube kubelet[1708]: E0523 00:15:58.876657    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959358849283933 - 1747959407131003859)"
May 23 00:15:58 minikube kubelet[1708]: E0523 00:15:58.876855    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959358858789233 - 1747959407131072517)"
May 23 00:15:58 minikube kubelet[1708]: E0523 00:15:58.876944    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959358858997533 - 1747959407130597501)"
May 23 00:15:58 minikube kubelet[1708]: E0523 00:15:58.877010    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959358859034133 - 1747959407130656442)"
May 23 00:15:58 minikube kubelet[1708]: E0523 00:15:58.877070    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959358859074933 - 1747959407130280059)"
May 23 00:15:58 minikube kubelet[1708]: E0523 00:15:58.877135    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959358859129833 - 1747959407129345242)"
May 23 00:15:58 minikube kubelet[1708]: E0523 00:15:58.877197    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959358859205633 - 1747959407129015701)"
May 23 00:15:58 minikube kubelet[1708]: E0523 00:15:58.877314    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959358859261033 - 1747959407134259859)"
May 23 00:15:58 minikube kubelet[1708]: E0523 00:15:58.877375    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959358859279133 - 1747959407130888542)"
May 23 00:15:58 minikube kubelet[1708]: E0523 00:15:58.877447    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959358859387433 - 1747959407129239459)"
May 23 00:15:58 minikube kubelet[1708]: E0523 00:15:58.877501    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959358859537933 - 1747959407127564434)"
May 23 00:15:58 minikube kubelet[1708]: E0523 00:15:58.877570    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959358859570833 - 1747959407133182317)"
May 23 00:15:58 minikube kubelet[1708]: E0523 00:15:58.877643    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959358859689833 - 1747959407128932009)"
May 23 00:15:58 minikube kubelet[1708]: E0523 00:15:58.877709    1708 cri_stats_provider.go:804] "Failed updating cpu usage nano core" err="zero or negative interval (1747959358862453533 - 1747959407130844451)"


==> kubernetes-dashboard [d281423bcef9] <==
2025/05/23 00:10:01 Using namespace: kubernetes-dashboard
2025/05/23 00:10:01 Using in-cluster config to connect to apiserver
2025/05/23 00:10:01 Using secret token for csrf signing
2025/05/23 00:10:01 Initializing csrf token from kubernetes-dashboard-csrf secret
2025/05/23 00:10:01 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2025/05/23 00:10:01 Successful initial request to the apiserver, version: v1.32.0
2025/05/23 00:10:01 Generating JWE encryption key
2025/05/23 00:10:01 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2025/05/23 00:10:01 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2025/05/23 00:10:02 Initializing JWE encryption key from synchronized object
2025/05/23 00:10:02 Creating in-cluster Sidecar client
2025/05/23 00:10:02 Successful request to sidecar
2025/05/23 00:10:02 Serving insecurely on HTTP port: 9090
2025/05/23 00:10:01 Starting overwatch


==> kubernetes-dashboard [f2deeac894ef] <==
2025/05/23 00:09:05 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": dial tcp 10.96.0.1:443: i/o timeout

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc00061fae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc0000e8100)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf
2025/05/23 00:09:05 Using namespace: kubernetes-dashboard
2025/05/23 00:09:05 Using in-cluster config to connect to apiserver
2025/05/23 00:09:05 Using secret token for csrf signing
2025/05/23 00:09:05 Initializing csrf token from kubernetes-dashboard-csrf secret


==> storage-provisioner [748a57e2d21a] <==
I0523 00:09:05.536253       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0523 00:09:38.382263       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [a846cd77b3ec] <==
I0523 00:10:06.540734       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0523 00:10:06.595204       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0523 00:10:06.597762       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0523 00:10:24.074475       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0523 00:10:24.074778       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"ed0725fc-e610-4648-9e74-41c5c24821fe", APIVersion:"v1", ResourceVersion:"21214", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_1e2fb5df-ce5b-4de9-b68e-60fed8b0da23 became leader
I0523 00:10:24.074993       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_1e2fb5df-ce5b-4de9-b68e-60fed8b0da23!
I0523 00:10:24.179409       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_1e2fb5df-ce5b-4de9-b68e-60fed8b0da23!

